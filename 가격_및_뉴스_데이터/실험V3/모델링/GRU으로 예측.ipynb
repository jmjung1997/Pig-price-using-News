{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b837da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf35aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import optim\n",
    "# 데이터 불러오기\n",
    "df_all=pd.read_csv('../data_v3/감성점수와 일별뉴스 합친거.csv',encoding='utf-8')\n",
    "df = df_all.iloc[:,[1,2,5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c736d",
   "metadata": {},
   "source": [
    "### 경락단가로 경락단가 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ba2a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:723: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:723: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:723: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:723: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train tensor([[[0.2916],\n",
      "         [0.2972],\n",
      "         [0.2346],\n",
      "         [0.2357]],\n",
      "\n",
      "        [[0.3749],\n",
      "         [0.3393],\n",
      "         [0.2689],\n",
      "         [0.3607]],\n",
      "\n",
      "        [[0.1328],\n",
      "         [0.1451],\n",
      "         [0.1378],\n",
      "         [0.1110]],\n",
      "\n",
      "        [[0.5970],\n",
      "         [0.6994],\n",
      "         [0.6986],\n",
      "         [0.6747]],\n",
      "\n",
      "        [[0.5014],\n",
      "         [0.4626],\n",
      "         [0.4731],\n",
      "         [0.4615]],\n",
      "\n",
      "        [[0.3603],\n",
      "         [1.0000],\n",
      "         [0.6796],\n",
      "         [0.6436]],\n",
      "\n",
      "        [[0.2298],\n",
      "         [0.1767],\n",
      "         [0.2170],\n",
      "         [0.1717]],\n",
      "\n",
      "        [[0.4089],\n",
      "         [0.4055],\n",
      "         [0.4041],\n",
      "         [0.3966]],\n",
      "\n",
      "        [[0.6222],\n",
      "         [0.6391],\n",
      "         [0.6541],\n",
      "         [0.5859]],\n",
      "\n",
      "        [[0.2906],\n",
      "         [0.3221],\n",
      "         [0.2916],\n",
      "         [0.2972]],\n",
      "\n",
      "        [[0.4547],\n",
      "         [0.5327],\n",
      "         [0.5214],\n",
      "         [0.4578]],\n",
      "\n",
      "        [[0.3843],\n",
      "         [0.3847],\n",
      "         [0.4258],\n",
      "         [0.3909]],\n",
      "\n",
      "        [[0.1916],\n",
      "         [0.2603],\n",
      "         [0.2781],\n",
      "         [0.2995]],\n",
      "\n",
      "        [[0.5309],\n",
      "         [0.5242],\n",
      "         [0.5593],\n",
      "         [0.5416]],\n",
      "\n",
      "        [[0.2178],\n",
      "         [0.2171],\n",
      "         [0.2287],\n",
      "         [0.2812]],\n",
      "\n",
      "        [[0.3884],\n",
      "         [0.3694],\n",
      "         [0.3945],\n",
      "         [0.3805]],\n",
      "\n",
      "        [[0.5324],\n",
      "         [0.5532],\n",
      "         [0.5281],\n",
      "         [0.5585]],\n",
      "\n",
      "        [[0.3847],\n",
      "         [0.4258],\n",
      "         [0.3909],\n",
      "         [0.3700]],\n",
      "\n",
      "        [[0.6138],\n",
      "         [0.6079],\n",
      "         [0.5747],\n",
      "         [0.6620]],\n",
      "\n",
      "        [[0.1866],\n",
      "         [0.2287],\n",
      "         [0.1339],\n",
      "         [0.1971]],\n",
      "\n",
      "        [[0.3437],\n",
      "         [0.3168],\n",
      "         [0.3760],\n",
      "         [0.3616]],\n",
      "\n",
      "        [[0.3254],\n",
      "         [0.3395],\n",
      "         [0.3292],\n",
      "         [0.3052]],\n",
      "\n",
      "        [[0.2603],\n",
      "         [0.2781],\n",
      "         [0.2995],\n",
      "         [0.3029]],\n",
      "\n",
      "        [[0.1028],\n",
      "         [0.1089],\n",
      "         [0.0951],\n",
      "         [0.0576]],\n",
      "\n",
      "        [[0.3633],\n",
      "         [0.3282],\n",
      "         [0.2906],\n",
      "         [0.3221]],\n",
      "\n",
      "        [[0.3606],\n",
      "         [0.3818],\n",
      "         [0.4207],\n",
      "         [0.4414]],\n",
      "\n",
      "        [[0.6145],\n",
      "         [0.6607],\n",
      "         [0.7003],\n",
      "         [0.6278]],\n",
      "\n",
      "        [[0.3551],\n",
      "         [0.3356],\n",
      "         [0.3603],\n",
      "         [1.0000]],\n",
      "\n",
      "        [[0.3700],\n",
      "         [0.4185],\n",
      "         [0.4060],\n",
      "         [0.3445]],\n",
      "\n",
      "        [[0.3202],\n",
      "         [0.3198],\n",
      "         [0.3233],\n",
      "         [0.3033]],\n",
      "\n",
      "        [[0.0875],\n",
      "         [0.0354],\n",
      "         [0.0980],\n",
      "         [0.0935]],\n",
      "\n",
      "        [[0.3760],\n",
      "         [0.3616],\n",
      "         [0.3772],\n",
      "         [0.3606]],\n",
      "\n",
      "        [[0.6079],\n",
      "         [0.5747],\n",
      "         [0.6620],\n",
      "         [0.6814]],\n",
      "\n",
      "        [[0.2781],\n",
      "         [0.2995],\n",
      "         [0.3029],\n",
      "         [0.2787]],\n",
      "\n",
      "        [[0.3772],\n",
      "         [0.3332],\n",
      "         [0.2573],\n",
      "         [0.3324]],\n",
      "\n",
      "        [[0.3116],\n",
      "         [0.3144],\n",
      "         [0.2706],\n",
      "         [0.2828]],\n",
      "\n",
      "        [[0.3754],\n",
      "         [0.3408],\n",
      "         [0.2979],\n",
      "         [0.3271]],\n",
      "\n",
      "        [[0.1451],\n",
      "         [0.1628],\n",
      "         [0.1916],\n",
      "         [0.2603]],\n",
      "\n",
      "        [[0.3345],\n",
      "         [0.3326],\n",
      "         [0.3884],\n",
      "         [0.3694]],\n",
      "\n",
      "        [[0.6031],\n",
      "         [0.6138],\n",
      "         [0.6079],\n",
      "         [0.5747]],\n",
      "\n",
      "        [[0.6278],\n",
      "         [0.7234],\n",
      "         [0.5970],\n",
      "         [0.6994]],\n",
      "\n",
      "        [[0.5000],\n",
      "         [0.4593],\n",
      "         [0.4675],\n",
      "         [0.4766]],\n",
      "\n",
      "        [[0.5747],\n",
      "         [0.6620],\n",
      "         [0.6814],\n",
      "         [0.6295]],\n",
      "\n",
      "        [[0.6394],\n",
      "         [0.6359],\n",
      "         [0.6689],\n",
      "         [0.6389]],\n",
      "\n",
      "        [[0.3090],\n",
      "         [0.3342],\n",
      "         [0.3295],\n",
      "         [0.3254]],\n",
      "\n",
      "        [[0.2925],\n",
      "         [0.2648],\n",
      "         [0.2507],\n",
      "         [0.3170]],\n",
      "\n",
      "        [[0.2534],\n",
      "         [0.3169],\n",
      "         [0.3090],\n",
      "         [0.3342]],\n",
      "\n",
      "        [[0.3945],\n",
      "         [0.3805],\n",
      "         [0.3850],\n",
      "         [0.3981]],\n",
      "\n",
      "        [[0.7632],\n",
      "         [0.7410],\n",
      "         [0.6542],\n",
      "         [0.6724]],\n",
      "\n",
      "        [[0.2180],\n",
      "         [0.2008],\n",
      "         [0.2178],\n",
      "         [0.2171]],\n",
      "\n",
      "        [[0.5327],\n",
      "         [0.5214],\n",
      "         [0.4578],\n",
      "         [0.4187]],\n",
      "\n",
      "        [[0.2775],\n",
      "         [0.2870],\n",
      "         [0.3009],\n",
      "         [0.3473]],\n",
      "\n",
      "        [[0.3773],\n",
      "         [0.3598],\n",
      "         [0.3031],\n",
      "         [0.3914]],\n",
      "\n",
      "        [[0.3295],\n",
      "         [0.3254],\n",
      "         [0.3395],\n",
      "         [0.3292]],\n",
      "\n",
      "        [[0.4615],\n",
      "         [0.5309],\n",
      "         [0.5242],\n",
      "         [0.5593]],\n",
      "\n",
      "        [[0.1969],\n",
      "         [0.1866],\n",
      "         [0.2287],\n",
      "         [0.1339]],\n",
      "\n",
      "        [[0.3759],\n",
      "         [0.3281],\n",
      "         [0.3508],\n",
      "         [0.3934]],\n",
      "\n",
      "        [[0.7165],\n",
      "         [0.7737],\n",
      "         [0.7495],\n",
      "         [0.7633]],\n",
      "\n",
      "        [[0.2357],\n",
      "         [0.1983],\n",
      "         [0.2596],\n",
      "         [0.3116]],\n",
      "\n",
      "        [[0.3009],\n",
      "         [0.3473],\n",
      "         [0.3332],\n",
      "         [0.3773]],\n",
      "\n",
      "        [[0.2834],\n",
      "         [0.3039],\n",
      "         [0.2612],\n",
      "         [0.2432]],\n",
      "\n",
      "        [[0.0680],\n",
      "         [0.0905],\n",
      "         [0.0531],\n",
      "         [0.0421]],\n",
      "\n",
      "        [[0.2792],\n",
      "         [0.3237],\n",
      "         [0.3354],\n",
      "         [0.2891]],\n",
      "\n",
      "        [[0.1279],\n",
      "         [0.1451],\n",
      "         [0.1628],\n",
      "         [0.1916]],\n",
      "\n",
      "        [[0.3818],\n",
      "         [0.4207],\n",
      "         [0.4414],\n",
      "         [0.4692]],\n",
      "\n",
      "        [[0.3350],\n",
      "         [0.3489],\n",
      "         [0.3754],\n",
      "         [0.3408]],\n",
      "\n",
      "        [[0.4976],\n",
      "         [0.5014],\n",
      "         [0.4626],\n",
      "         [0.4731]],\n",
      "\n",
      "        [[0.6814],\n",
      "         [0.6295],\n",
      "         [0.6145],\n",
      "         [0.6607]],\n",
      "\n",
      "        [[0.0935],\n",
      "         [0.0979],\n",
      "         [0.1681],\n",
      "         [0.2026]],\n",
      "\n",
      "        [[0.3262],\n",
      "         [0.3437],\n",
      "         [0.3168],\n",
      "         [0.3760]],\n",
      "\n",
      "        [[0.3694],\n",
      "         [0.3945],\n",
      "         [0.3805],\n",
      "         [0.3850]],\n",
      "\n",
      "        [[0.3030],\n",
      "         [0.3202],\n",
      "         [0.3198],\n",
      "         [0.3233]],\n",
      "\n",
      "        [[0.7003],\n",
      "         [0.6278],\n",
      "         [0.7234],\n",
      "         [0.5970]],\n",
      "\n",
      "        [[0.3033],\n",
      "         [0.2459],\n",
      "         [0.2878],\n",
      "         [0.2275]],\n",
      "\n",
      "        [[0.0421],\n",
      "         [0.0636],\n",
      "         [0.0731],\n",
      "         [0.0141]],\n",
      "\n",
      "        [[0.3198],\n",
      "         [0.3233],\n",
      "         [0.3033],\n",
      "         [0.2459]],\n",
      "\n",
      "        [[0.3760],\n",
      "         [0.4338],\n",
      "         [0.4724],\n",
      "         [0.4794]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0384],\n",
      "         [0.0498],\n",
      "         [0.0911]],\n",
      "\n",
      "        [[0.5281],\n",
      "         [0.5585],\n",
      "         [0.5108],\n",
      "         [0.5284]],\n",
      "\n",
      "        [[0.7578],\n",
      "         [0.7662],\n",
      "         [0.7401],\n",
      "         [0.7301]],\n",
      "\n",
      "        [[0.1908],\n",
      "         [0.1871],\n",
      "         [0.1984],\n",
      "         [0.2129]],\n",
      "\n",
      "        [[0.2215],\n",
      "         [0.2227],\n",
      "         [0.2298],\n",
      "         [0.1767]],\n",
      "\n",
      "        [[0.1971],\n",
      "         [0.1991],\n",
      "         [0.1279],\n",
      "         [0.1451]],\n",
      "\n",
      "        [[0.6835],\n",
      "         [0.5778],\n",
      "         [0.5813],\n",
      "         [0.5151]],\n",
      "\n",
      "        [[0.4950],\n",
      "         [0.5000],\n",
      "         [0.4593],\n",
      "         [0.4675]],\n",
      "\n",
      "        [[0.6724],\n",
      "         [0.6241],\n",
      "         [0.4874],\n",
      "         [0.4828]],\n",
      "\n",
      "        [[0.2346],\n",
      "         [0.2357],\n",
      "         [0.1983],\n",
      "         [0.2596]],\n",
      "\n",
      "        [[0.5813],\n",
      "         [0.5151],\n",
      "         [0.4216],\n",
      "         [0.4056]],\n",
      "\n",
      "        [[0.0911],\n",
      "         [0.0309],\n",
      "         [0.1158],\n",
      "         [0.1688]],\n",
      "\n",
      "        [[0.3607],\n",
      "         [0.3350],\n",
      "         [0.3489],\n",
      "         [0.3754]],\n",
      "\n",
      "        [[0.4356],\n",
      "         [0.4673],\n",
      "         [0.2316],\n",
      "         [0.2545]],\n",
      "\n",
      "        [[0.2878],\n",
      "         [0.2275],\n",
      "         [0.2057],\n",
      "         [0.1982]],\n",
      "\n",
      "        [[0.5815],\n",
      "         [0.5707],\n",
      "         [0.6051],\n",
      "         [0.5882]],\n",
      "\n",
      "        [[0.3221],\n",
      "         [0.2916],\n",
      "         [0.2972],\n",
      "         [0.2346]],\n",
      "\n",
      "        [[0.2891],\n",
      "         [0.3168],\n",
      "         [0.2544],\n",
      "         [0.2666]],\n",
      "\n",
      "        [[0.1457],\n",
      "         [0.0938],\n",
      "         [0.0045],\n",
      "         [0.0026]],\n",
      "\n",
      "        [[0.4692],\n",
      "         [0.4683],\n",
      "         [0.4365],\n",
      "         [0.5054]],\n",
      "\n",
      "        [[0.7401],\n",
      "         [0.7301],\n",
      "         [0.7516],\n",
      "         [0.7723]],\n",
      "\n",
      "        [[0.4593],\n",
      "         [0.4675],\n",
      "         [0.4766],\n",
      "         [0.4905]],\n",
      "\n",
      "        [[0.5554],\n",
      "         [0.5138],\n",
      "         [0.4911],\n",
      "         [0.5265]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6940\\181997621.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;31m# 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;31m# epoch별 손실값\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6940\\181997621.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_df, num_epochs, lr, verbose, patience)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# H(x) 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;31m# cost 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6940\\181997621.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mh_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0mhn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jm\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jm\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 740\u001b[1;33m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
     ]
    }
   ],
   "source": [
    "#df=df[['day_sentiment','경락단가_평균']]\n",
    "df=df[['경락단가','경락단가_평균']]\n",
    "\n",
    "df.tail()\n",
    "\n",
    "# 4주의 데이터가 입력으로 들어가고 batch size는 임의로 지정\n",
    "seq_length = 4\n",
    "batch = 100\n",
    "\n",
    "# 데이터를 역순으로 정렬하여 전체 데이터의 70% 학습, 30% 테스트에 사용\n",
    "train_size = int(len(df)*0.7)\n",
    "train_set = df[0:train_size]  \n",
    "test_set = df[train_size-seq_length:]\n",
    "\n",
    "# Input scale\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(train_set.iloc[:, :-1])\n",
    "\n",
    "train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n",
    "test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:, :-1])\n",
    "\n",
    "# Output scale\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(train_set.iloc[:, [-1]])\n",
    "\n",
    "train_set.iloc[:, -1:] = scaler_y.transform(train_set.iloc[:, -1:])\n",
    "test_set.iloc[:, -1:] = scaler_y.transform(test_set.iloc[:, -1:])\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series)-seq_length):\n",
    "        _x = time_series[i:i+seq_length, :-1]\n",
    "        _y = time_series[i+seq_length, [-1]]\n",
    "        # print(_x, \"-->\",_y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(np.array(train_set), seq_length)\n",
    "testX, testY = build_dataset(np.array(test_set), seq_length)\n",
    "\n",
    "# 텐서로 변환\n",
    "trainX_tensor = torch.FloatTensor(trainX)\n",
    "\n",
    "trainY_tensor = torch.FloatTensor(trainY)\n",
    "\n",
    "testX_tensor = torch.FloatTensor(testX)\n",
    "testY_tensor = torch.FloatTensor(testY)\n",
    "\n",
    "# 텐서 형태로 데이터 정의\n",
    "dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
    "\n",
    "# 데이터로더는 기본적으로 2개의 인자를 입력받으며 배치크기는 통상적으로 2의 배수를 사용\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=True,  \n",
    "                        drop_last=True)\n",
    "\n",
    "# 설정값\n",
    "data_dim = 1\n",
    "hidden_dim = 10 \n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 100\n",
    "class GRU(nn.Module) :\n",
    "    def __init__(self, input_size, hidden_size, seq_length ,num_classes,num_layers) :\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size,\n",
    "                         num_layers=num_layers,batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "     # 학습 초기화를 위한 함수\n",
    "    def reset_hidden_state(self): \n",
    "        self.hidden = (\n",
    "                torch.zeros(self.num_layers, self.seq_length, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, self.seq_length, self.hidden_size))\n",
    "    \n",
    "        \n",
    "    def forward(self, x) :\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        output, (hn) = self.gru(x, (h_0))\n",
    "        hn = hn.view(-1, self.hidden_size)\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "def train_model(model, train_df, num_epochs = None, lr = None, verbose = 10, patience = 10):\n",
    "     \n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    nb_epochs = num_epochs\n",
    "    \n",
    "    # epoch마다 loss 저장\n",
    "    train_hist = np.zeros(nb_epochs)\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = len(train_df)\n",
    "        \n",
    "        for batch_idx, samples in enumerate(train_df):\n",
    "\n",
    "            x_train, y_train = samples\n",
    "            print(\"x_train\",x_train)\n",
    "            # seq별 hidden state reset\n",
    "            model.reset_hidden_state()\n",
    "            \n",
    "            # H(x) 계산\n",
    "            outputs = model(x_train)\n",
    "                \n",
    "            # cost 계산\n",
    "            loss = criterion(outputs, y_train)                    \n",
    "            \n",
    "            # cost로 H(x) 개선\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_cost += loss/total_batch\n",
    "               \n",
    "        train_hist[epoch] = avg_cost        \n",
    "        \n",
    "        if epoch % verbose == 0:\n",
    "            print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n",
    "            \n",
    "        # patience번째 마다 early stopping 여부 확인\n",
    "        if (epoch % patience == 0) & (epoch != 0):\n",
    "            \n",
    "            # loss가 커졌다면 early stop\n",
    "            if train_hist[epoch-patience] < train_hist[epoch]:\n",
    "                print('\\n Early Stopping')\n",
    "                \n",
    "                break\n",
    "            \n",
    "    return model.eval(), train_hist\n",
    "\n",
    "# 모델 학습\n",
    "net = GRU(data_dim, hidden_dim, seq_length, output_dim, 2).to(device)  \n",
    "model, train_hist = train_model(net, dataloader, num_epochs = nb_epochs, lr = learning_rate, verbose = 20, patience = 10)\n",
    "\n",
    "# epoch별 손실값\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 저장    \n",
    "PATH = \"./Timeseries_GRU_data-02-stock_daily_2.pth\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "model = GRU(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)  \n",
    "model.load_state_dict(torch.load(PATH), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 테스트\n",
    "with torch.no_grad(): \n",
    "    pred = []\n",
    "    for pr in range(len(testX_tensor)):\n",
    "\n",
    "        model.reset_hidden_state()\n",
    "\n",
    "        predicted = model(torch.unsqueeze(testX_tensor[pr], 0))\n",
    "        predicted = torch.flatten(predicted).item()        pred.append(predicted)\n",
    "\n",
    "    # INVERSE\n",
    "    pred_inverse = scaler_y.inverse_transform(np.array(pred).reshape(-1, 1))\n",
    "    testY_inverse = scaler_y.inverse_transform(testY_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(true, pred):\n",
    "    return np.mean(np.abs(true-pred))\n",
    "\n",
    "print('MAE SCORE : ', MAE(pred_inverse, testY_inverse))\n",
    "\n",
    "#MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"MAE\",mean_absolute_error(testY_inverse,pred_inverse))\n",
    "\n",
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE\",mean_squared_error(testY_inverse,pred_inverse))\n",
    "\n",
    "#RMSE\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MSE = mean_squared_error(testY_inverse,pred_inverse)\n",
    "print(\"RMSE\",np.sqrt(MSE))\n",
    "# sklearn 은 mse만 제공하기 때문에 rmse는 직접 만들어 써야한다.\n",
    "\n",
    "#MAPE\n",
    "import numpy as np\n",
    "\n",
    "def MAPE(y_test, y_pred):\n",
    "\treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    \n",
    "print(\"MAPE\",MAPE(testY_inverse, pred_inverse))\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(np.arange(len(pred_inverse)), pred_inverse, label = 'pred')\n",
    "plt.plot(np.arange(len(testY_inverse)), testY_inverse, label = 'true')\n",
    "plt.title(\"Loss plot\")\n",
    "plt.legend()\n",
    "plt.xlabel('Week')\n",
    "\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1660119",
   "metadata": {},
   "source": [
    "### 도매가격+감성점수-> 도매가격"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceac0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df_all=pd.read_csv('../data_v3/감성점수와 일별뉴스 합친거.csv',encoding='utf-8')\n",
    "df = df_all.iloc[:,[1,2,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757906d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df[['day_sentiment','경락단가_평균']]\n",
    "df=df[['day_sentiment','경락단가_평균']]\n",
    "\n",
    "df.tail()\n",
    "\n",
    "# 4주간격 데이터가 입력으로 들어가고 batch size는 임의로 지정\n",
    "seq_length = 4\n",
    "batch = 100\n",
    "\n",
    "# 데이터를 역순으로 정렬하여 전체 데이터의 70% 학습, 30% 테스트에 사용\n",
    "train_size = int(len(df)*0.7)\n",
    "train_set = df[0:train_size]  \n",
    "test_set = df[train_size-seq_length:]\n",
    "\n",
    "# Input scale\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(train_set.iloc[:, :-1])\n",
    "\n",
    "train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n",
    "test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:, :-1])\n",
    "\n",
    "# Output scale\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(train_set.iloc[:, [-1]])\n",
    "\n",
    "train_set.iloc[:, -1:] = scaler_y.transform(train_set.iloc[:, -1:])\n",
    "test_set.iloc[:, -1:] = scaler_y.transform(test_set.iloc[:, -1:])\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series)-seq_length):\n",
    "        _x = time_series[i:i+seq_length, :]\n",
    "        _y = time_series[i+seq_length, [-1]]\n",
    "        # print(_x, \"-->\",_y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(np.array(train_set), seq_length)\n",
    "testX, testY = build_dataset(np.array(test_set), seq_length)\n",
    "\n",
    "# 텐서로 변환\n",
    "trainX_tensor = torch.FloatTensor(trainX)\n",
    "trainY_tensor = torch.FloatTensor(trainY)\n",
    "\n",
    "testX_tensor = torch.FloatTensor(testX)\n",
    "testY_tensor = torch.FloatTensor(testY)\n",
    "\n",
    "# 텐서 형태로 데이터 정의\n",
    "dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
    "\n",
    "# 데이터로더는 기본적으로 2개의 인자를 입력받으며 배치크기는 통상적으로 2의 배수를 사용\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=True,  \n",
    "                        drop_last=True)\n",
    "\n",
    "# 설정값\n",
    "data_dim = 2\n",
    "hidden_dim = 10 \n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 100\n",
    "class GRU(nn.Module) :\n",
    "    def __init__(self, input_size, hidden_size, seq_length ,num_classes,num_layers) :\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size,\n",
    "                         num_layers=num_layers,batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "     # 학습 초기화를 위한 함수\n",
    "    def reset_hidden_state(self): \n",
    "        self.hidden = (\n",
    "                torch.zeros(self.num_layers, self.seq_length, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, self.seq_length, self.hidden_size))\n",
    "    \n",
    "        \n",
    "    def forward(self, x) :\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        output, (hn) = self.gru(x, (h_0))\n",
    "        hn = hn.view(-1, self.hidden_size)\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "def train_model(model, train_df, num_epochs = None, lr = None, verbose = 10, patience = 10):\n",
    "     \n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    nb_epochs = num_epochs\n",
    "    \n",
    "    # epoch마다 loss 저장\n",
    "    train_hist = np.zeros(nb_epochs)\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = len(train_df)\n",
    "        \n",
    "        for batch_idx, samples in enumerate(train_df):\n",
    "\n",
    "            x_train, y_train = samples\n",
    "            print(\"x_train\",x_train)\n",
    "            # seq별 hidden state reset\n",
    "            model.reset_hidden_state()\n",
    "            \n",
    "            # H(x) 계산\n",
    "            outputs = model(x_train)\n",
    "                \n",
    "            # cost 계산\n",
    "            loss = criterion(outputs, y_train)                    \n",
    "            \n",
    "            # cost로 H(x) 개선\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_cost += loss/total_batch\n",
    "               \n",
    "        train_hist[epoch] = avg_cost        \n",
    "        \n",
    "        if epoch % verbose == 0:\n",
    "            print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n",
    "            \n",
    "        # patience번째 마다 early stopping 여부 확인\n",
    "        if (epoch % patience == 0) & (epoch != 0):\n",
    "            \n",
    "            # loss가 커졌다면 early stop\n",
    "            if train_hist[epoch-patience] < train_hist[epoch]:\n",
    "                print('\\n Early Stopping')\n",
    "                \n",
    "                break\n",
    "            \n",
    "    return model.eval(), train_hist\n",
    "\n",
    "# 모델 학습\n",
    "net = GRU(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)  \n",
    "model, train_hist = train_model(net, dataloader, num_epochs = nb_epochs, lr = learning_rate, verbose = 20, patience = 10)\n",
    "\n",
    "# epoch별 손실값\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 저장    \n",
    "PATH = \"./Timeseries_GRU_data-02-stock_daily_4.pth\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "model = GRU(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)  \n",
    "model.load_state_dict(torch.load(PATH), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c43b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 테스트\n",
    "with torch.no_grad(): \n",
    "    pred = []\n",
    "    for pr in range(len(testX_tensor)):\n",
    "\n",
    "        model.reset_hidden_state()\n",
    "\n",
    "        predicted = model(torch.unsqueeze(testX_tensor[pr], 0))\n",
    "        predicted = torch.flatten(predicted).item()\n",
    "        pred.append(predicted)\n",
    "\n",
    "    # INVERSE\n",
    "    pred_inverse = scaler_y.inverse_transform(np.array(pred).reshape(-1, 1))\n",
    "    testY_inverse = scaler_y.inverse_transform(testY_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(true, pred):\n",
    "    return np.mean(np.abs(true-pred))\n",
    "\n",
    "print('MAE SCORE : ', MAE(pred_inverse, testY_inverse))\n",
    "\n",
    "#MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"MAE\",mean_absolute_error(testY_inverse,pred_inverse))\n",
    "\n",
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE\",mean_squared_error(testY_inverse,pred_inverse))\n",
    "\n",
    "#RMSE\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MSE = mean_squared_error(testY_inverse,pred_inverse)\n",
    "print(\"RMSE\",np.sqrt(MSE))\n",
    "# sklearn 은 mse만 제공하기 때문에 rmse는 직접 만들어 써야한다.\n",
    "\n",
    "#MAPE\n",
    "import numpy as np\n",
    "\n",
    "def MAPE(y_test, y_pred):\n",
    "\treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    \n",
    "print(\"MAPE\",MAPE(testY_inverse, pred_inverse))\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(np.arange(len(pred_inverse)), pred_inverse, label = 'pred')\n",
    "plt.plot(np.arange(len(testY_inverse)), testY_inverse, label = 'true')\n",
    "plt.title(\"Loss plot\")\n",
    "plt.legend()\n",
    "plt.xlabel('Week')\n",
    "\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c7472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ac5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72654948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
