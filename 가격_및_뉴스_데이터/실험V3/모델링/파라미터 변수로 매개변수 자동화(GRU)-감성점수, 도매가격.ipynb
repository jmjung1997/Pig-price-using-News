{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20df78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b8045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import optim\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# 데이터 불러오기\n",
    "df_all=pd.read_csv('../data_v3/감성점수와 일별뉴스 합친거.csv',encoding='utf-8')\n",
    "df = df_all.iloc[:,[1,2,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17dd4566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype={value.dtype})\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>경락단가</th>\n",
       "      <th>도매가격</th>\n",
       "      <th>day_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3750.366093</td>\n",
       "      <td>4104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3547.574932</td>\n",
       "      <td>4049</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3696.060897</td>\n",
       "      <td>3920</td>\n",
       "      <td>-0.104167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3770.156364</td>\n",
       "      <td>3917</td>\n",
       "      <td>-0.104167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3809.974684</td>\n",
       "      <td>4090</td>\n",
       "      <td>-0.104167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>4547.071429</td>\n",
       "      <td>4718</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>4373.212544</td>\n",
       "      <td>4749</td>\n",
       "      <td>-0.116270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>4278.649485</td>\n",
       "      <td>4820</td>\n",
       "      <td>-0.116270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>4322.080986</td>\n",
       "      <td>4919</td>\n",
       "      <td>-0.116270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>4138.037190</td>\n",
       "      <td>4988</td>\n",
       "      <td>-0.116270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>625 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            경락단가  도매가격  day_sentiment\n",
       "0    3750.366093  4104       0.000000\n",
       "1    3547.574932  4049       0.000000\n",
       "2    3696.060897  3920      -0.104167\n",
       "3    3770.156364  3917      -0.104167\n",
       "4    3809.974684  4090      -0.104167\n",
       "..           ...   ...            ...\n",
       "620  4547.071429  4718       0.175000\n",
       "621  4373.212544  4749      -0.116270\n",
       "622  4278.649485  4820      -0.116270\n",
       "623  4322.080986  4919      -0.116270\n",
       "624  4138.037190  4988      -0.116270\n",
       "\n",
       "[625 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "df_all=pd.read_csv('../data_v3/감성점수와 일별뉴스 합친거.csv',encoding='utf-8')\n",
    "df = df_all.iloc[:,[1,2,5]]\n",
    "\n",
    "df_all=pd.read_excel('../data_v3/전국경락단가.xlsx')\n",
    "\n",
    "df_all\n",
    "\n",
    "df['경락단가_평균']=df_all['price']\n",
    "\n",
    "df_all\n",
    "\n",
    "df.columns=['경락단가', '도매가격', 'day_sentiment']\n",
    "\n",
    "df\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8a9eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype={value.dtype})\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "df_all=pd.read_csv('../data_v3/감성점수와 일별뉴스 합친거.csv',encoding='utf-8')\n",
    "df = df_all.iloc[:,[1,2,5]]\n",
    "df_all=pd.read_excel('../data_v3/전국경락단가.xlsx')\n",
    "df['경락단가_평균']=df_all['price']\n",
    "df.columns=['경락단가', '도매가격', 'day_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26d2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=df[['도매가격','day_sentiment','도매가격']]\n",
    "title='도매가격_감성점수'\n",
    "term_np=np.array([])\n",
    "MAE_np=np.array([])\n",
    "MSE_np=np.array([])\n",
    "RMSE_np=np.array([])\n",
    "MAPE_np=np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76673908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2train_size-2predict_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:1835: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:1835: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:1835: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\jaemin\\Anaconda3\\envs\\jm\\lib\\site-packages\\pandas\\core\\indexing.py:1835: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22424\\927154130.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LEARNING_RATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0minfer_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22424\\927154130.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'how_inputdim'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hidden_dim'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PREDICT_SIZE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(2,22):\n",
    "    for j in range(2,8):\n",
    "        # 5일간의 데이터가 입력으로 들어가고 batch size는 임의로 지정\n",
    "        CFG={'TRAIN_WINDOW_SIZE':i,\n",
    "           'PREDICT_SIZE':j ,\n",
    "            'how_inputdim':2,\n",
    "            'hidden_dim':10,\n",
    "            'LEARNING_RATE':0.001,\n",
    "            'nb_epochs':100,\n",
    "            'BATCH_SIZE':25}\n",
    "        term=\"{}train_size-{}predict_size\".format(CFG['TRAIN_WINDOW_SIZE'],CFG['PREDICT_SIZE'])\n",
    "        print(term)\n",
    "    \n",
    "        # 데이터를 역순으로 정렬하여 전체 데이터의 70% 학습, 30% 테스트에 사용\n",
    "        train_size = int(len(df)*0.7)\n",
    "        train_set = df_data[0:train_size]  \n",
    "        test_set = df_data[train_size-CFG['TRAIN_WINDOW_SIZE']:]\n",
    "\n",
    "        # Input scale\n",
    "        scaler_x = MinMaxScaler()\n",
    "        scaler_x.fit(train_set.iloc[:, :-1])\n",
    "\n",
    "        train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n",
    "        test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:, :-1])\n",
    "\n",
    "        # Output scale\n",
    "        scaler_y = MinMaxScaler()\n",
    "        scaler_y.fit(train_set.iloc[:, [-1]])\n",
    "\n",
    "        train_set.iloc[:, -1:] = scaler_y.transform(train_set.iloc[:, -1:])\n",
    "        test_set.iloc[:, -1:] = scaler_y.transform(test_set.iloc[:, -1:])\n",
    "\n",
    "\n",
    "        # 데이터셋 생성 함수\n",
    "        def build_dataset(time_series,train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "            dataX = []\n",
    "            dataY = []\n",
    "            window_size = train_size + predict_size\n",
    "            for i in range(len(time_series) - window_size + 1):\n",
    "                _x = time_series[i:i+train_size, :-1]\n",
    "                _y = time_series[i+train_size:i+train_size+predict_size, [-1]]\n",
    "                # print(_x, \"-->\",_y)\n",
    "                dataX.append(_x)\n",
    "                dataY.append(_y)\n",
    "\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "\n",
    "        trainX, trainY = build_dataset(np.array(train_set))\n",
    "        testX, testY = build_dataset(np.array(test_set))\n",
    "\n",
    "        class CustomDataset(Dataset):\n",
    "            def __init__(self, X, Y):\n",
    "                self.X = X\n",
    "                self.Y = Y\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                if self.Y is not None:\n",
    "                    return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "                return torch.Tensor(self.X[index])\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.X)\n",
    "\n",
    "        train_dataset = CustomDataset(trainX, trainY)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "        val_dataset = CustomDataset(testX, testY)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "        # test_dataset = CustomDataset(test_input, None)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "        # 모델선언\n",
    "\n",
    "        class BaseModel(nn.Module):\n",
    "            def __init__(self, input_size=CFG['how_inputdim'], hidden_size=CFG['hidden_dim'], output_size=CFG['PREDICT_SIZE']):\n",
    "                super(BaseModel, self).__init__()\n",
    "                self.num_classes = num_classes\n",
    "                self.num_layers = num_layers\n",
    "                self.input_size = input_size\n",
    "                self.hidden_size = hidden_size\n",
    "                self.seq_length = seq_length\n",
    "\n",
    "                self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size,\n",
    "                                 num_layers=num_layers,batch_first=True)\n",
    "                self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "                self.fc = nn.Linear(128, num_classes)\n",
    "                self.relu = nn.ReLU()\n",
    "\n",
    "            def forward(self, x):\n",
    "                h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "                output, (hn) = self.gru(x, (h_0))\n",
    "                hn = hn.view(-1, self.hidden_size)\n",
    "                out = self.relu(hn)\n",
    "                out = self.fc_1(out)\n",
    "                out = self.relu(out)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "            def init_hidden(self, batch_size, device):\n",
    "                # Initialize hidden state and cell state\n",
    "                return (torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                        torch.zeros(1, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "        # 모델학습\n",
    "        \n",
    "        def train(model, optimizer, train_loader, val_loader, device):\n",
    "            model.to(device)\n",
    "            criterion = nn.MSELoss().to(device)\n",
    "            best_loss = 9999999\n",
    "            best_model = None\n",
    "\n",
    "            for epoch in range(1, CFG['nb_epochs']+1):\n",
    "                model.train()\n",
    "                train_loss = []\n",
    "                train_mae = []\n",
    "                for X, Y in tqdm(iter(train_loader)):\n",
    "                    X = X.to(device)\n",
    "                    Y = Y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    output = model(X)\n",
    "                    loss = criterion(output, Y.squeeze())\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss.append(loss.item())\n",
    "\n",
    "                val_loss = validation(model, val_loader, criterion, device)\n",
    "                print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "\n",
    "                if best_loss > val_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_model = model\n",
    "                    print('Model Saved')\n",
    "            return best_model\n",
    "\n",
    "        def validation(model, val_loader, criterion, device):\n",
    "            model.eval()\n",
    "            val_loss = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for X, Y in tqdm(iter(val_loader)):\n",
    "                    X = X.to(device)\n",
    "                    Y = Y.to(device)\n",
    "\n",
    "                    output = model(X)\n",
    "                    loss = criterion(output, Y.squeeze())\n",
    "\n",
    "                    val_loss.append(loss.item())\n",
    "            return np.mean(val_loss)\n",
    "\n",
    "        model = BaseModel()\n",
    "        optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "        infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "        def inference(model, test_loader, device):\n",
    "            model.eval()\n",
    "            predictions=[]\n",
    "            with torch.no_grad():\n",
    "                for X, Y in tqdm(iter(test_loader)):\n",
    "                    X = X.to(device)\n",
    "                    Y = Y.to(device)\n",
    "\n",
    "                    output = model(X)\n",
    "                    # 모델 출력인 output을 CPU로 이동하고 numpy 배열로 변환\n",
    "                    output = output.cpu().numpy()\n",
    "                    predictions.extend(output)\n",
    "\n",
    "            return np.array(predictions)\n",
    "\n",
    "        answer_np = np.empty((0,CFG['PREDICT_SIZE']))\n",
    "        \n",
    "        for val in val_loader:\n",
    "            answer_np = np.vstack((answer_np, val[1].view(-1,CFG['PREDICT_SIZE'])))\n",
    "\n",
    "        pred = inference(infer_model,val_loader , device)\n",
    "        print(pred.shape)\n",
    "\n",
    "        pred=scaler_y.inverse_transform(pred)\n",
    "     \n",
    "        # 눤래 정답을 inverse scaling\n",
    "\n",
    "        answer_np=scaler_y.inverse_transform(answer_np)    \n",
    "        # 결과 후처리\n",
    "        answer_np = np.round(answer_np, 0).astype(int)\n",
    "        print(\"answer_np\", answer_np)\n",
    "\n",
    "        # 결과 출력\n",
    "        #MAE\n",
    "        def MAE(true, pred):\n",
    "            return np.mean(np.abs(true-pred))\n",
    "\n",
    "        print('MAE SCORE : ', MAE(answer_np, pred))\n",
    "        MAE_SCORE=MAE(answer_np, pred)\n",
    "\n",
    "        #MSE\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "\n",
    "        print(\"MSE\",mean_squared_error(answer_np,pred))\n",
    "        MSE=mean_squared_error(answer_np,pred)\n",
    "        #RMSE\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "\n",
    "        MSE = mean_squared_error(answer_np,pred)\n",
    "        print(\"RMSE\",np.sqrt(MSE))\n",
    "        # sklearn 은 mse만 제공하기 때문에 rmse는 직접 만들어 써야한다.\n",
    "        RMSE=np.sqrt(MSE)\n",
    "        #MAPE\n",
    "        import numpy as np\n",
    "\n",
    "        def MAPE(y_test, y_pred):\n",
    "            return np.mean(np.abs((answer_np - pred) / answer_np)) * 100\n",
    "\n",
    "        print(\"MAPE\",MAPE(answer_np, pred))\n",
    "        MAPE_SCORE=MAPE(answer_np, pred)\n",
    "\n",
    "        #결과값 데이터프레임으로 저장\n",
    "        term_np=np.append(term_np,term)\n",
    "        MAE_np=np.append(MAE_np,MAE_SCORE )\n",
    "        MSE_np=np.append(MSE_np,MSE )\n",
    "        RMSE_np=np.append(RMSE_np,RMSE )\n",
    "        MAPE_np=np.append(MAPE_np,MAPE_SCORE )\n",
    "\n",
    "        # 그래프 그리기\n",
    "        answer=np.mean(answer_np, axis=1)\n",
    "        prediction = np.mean(pred, axis=1)\n",
    "\n",
    "        fig = plt.figure(figsize=(8,3))\n",
    "        plt.plot(np.arange(len(answer)), answer, label = 'true')\n",
    "        plt.plot(np.arange(len(prediction)), prediction, label = 'pred')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        plt.savefig(\"../결과/GRU_감성점수_graph/{} {}train_size {}predict_size.png\".format(title,CFG['TRAIN_WINDOW_SIZE'],CFG['PREDICT_SIZE']), dpi=300)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2d198",
   "metadata": {},
   "source": [
    "### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebdf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['term']=term_np\n",
    "df_result['MAE']=MAE_np\n",
    "df_result['MSE']=MSE_np\n",
    "df_result['RMSE']=RMSE_np\n",
    "df_result['MAPE']=MAPE_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(\"../결과/GRU_감성점수_result/result.csv\".format(title,CFG['TRAIN_WINDOW_SIZE'],CFG['PREDICT_SIZE']),encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36086284",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03965383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb5755",
   "metadata": {},
   "outputs": [],
   "source": [
    " for val in val_loader:\n",
    "    print(val[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c336aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c23d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
