{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b837da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf35aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import optim\n",
    "# 데이터 불러오기\n",
    "df_all=pd.read_csv('../data_v3/감성점수와 일별뉴스 합친거.csv',encoding='utf-8')\n",
    "df = df_all.iloc[:,[1,2,5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96ae30",
   "metadata": {},
   "source": [
    "### 경락단가로 경락단가 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e24fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df[['day_sentiment','경락단가_평균']]\n",
    "df=df[['경락단가','경락단가_평균']]\n",
    "\n",
    "df.tail()\n",
    "\n",
    "# 7일간의 데이터가 입력으로 들어가고 batch size는 임의로 지정\n",
    "seq_length = 5\n",
    "batch = 100\n",
    "\n",
    "# 데이터를 역순으로 정렬하여 전체 데이터의 70% 학습, 30% 테스트에 사용\n",
    "train_size = int(len(df)*0.7)\n",
    "train_set = df[0:train_size]  \n",
    "test_set = df[train_size-seq_length:]\n",
    "\n",
    "# # Input scale\n",
    "# scaler_x = MinMaxScaler()\n",
    "# scaler_x.fit(train_set.iloc[:, :-1])\n",
    "\n",
    "# train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n",
    "# test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:, :-1])\n",
    "\n",
    "# # Output scale\n",
    "# scaler_y = MinMaxScaler()\n",
    "# scaler_y.fit(train_set.iloc[:, [-1]])\n",
    "\n",
    "# train_set.iloc[:, -1:] = scaler_y.transform(train_set.iloc[:, -1:])\n",
    "# test_set.iloc[:, -1:] = scaler_y.transform(test_set.iloc[:, -1:])\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "\n",
    "# 설정값\n",
    "data_dim = 1\n",
    "hidden_dim = 10 \n",
    "global output_dim\n",
    "output_dim= 4\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 100\n",
    "# 데이터셋 생성 함수\n",
    "def build_dataset(time_series, seq_length,output_dim):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(0, len(time_series)-seq_length):\n",
    "        X_train.append(time_series[i:i+seq_length, :-1])\n",
    "        if len(time_series[i+seq_length:i+seq_length+output_dim, -1])!=output_dim:\n",
    "            new_list=[]\n",
    "            alist=time_series[i+seq_length:i+seq_length+output_dim, -1]\n",
    "            print(\"alist----------------\",alist)\n",
    "            alist = np.concatenate((alist, np.array([alist[-1]] * (output_dim- len(time_series[i+seq_length:i+seq_length+output_dim, -1])))))\n",
    "            y_train.append(alist)\n",
    "            alist=[]\n",
    "            new_list=[]\n",
    "        else:\n",
    "            y_train.append(time_series[i+seq_length:i+seq_length+output_dim, -1])\n",
    "            #_y = time_series[i+seq_length:i+seq_length+output_dim , [-1]]\n",
    "        # print(_x, \"-->\",_y)\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)   \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254adae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alist---------------- [3729.50909091 3839.60180995 3846.46728972]\n",
      "alist---------------- [3839.60180995 3846.46728972]\n",
      "alist---------------- [3846.46728972]\n",
      "alist---------------- [4278.64948454 4322.08098592 4138.03719008]\n",
      "alist---------------- [4322.08098592 4138.03719008]\n",
      "alist---------------- [4138.03719008]\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = build_dataset(np.array(train_set), seq_length,output_dim)\n",
    "testX, testY = build_dataset(np.array(test_set), seq_length,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1421da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrainX=trainX.reshape(-1,1)#(432, 5, 1)\n",
    "retestX=testX.reshape(-1,1)#(188, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692b2a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input scale\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(retrainX)\n",
    "\n",
    "retrainX = scaler_x.transform(retrainX)\n",
    "retestX = scaler_x.transform(retestX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a210d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=retrainX.reshape(trainX.shape)\n",
    "testX=retestX.reshape(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a8a9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(432, 5, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4680b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrainY=trainY.reshape(-1,1)#(432, 4)\n",
    "retestY=testY.reshape(-1,1)#(188, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ffd945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output scale\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(retrainX)\n",
    "\n",
    "retrainY = scaler_y.transform(retrainY)\n",
    "retestY = scaler_y.transform(retestY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad3a6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY=retrainY.reshape(trainY.shape)\n",
    "testY=retestY.reshape(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dafafe4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6d7e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환\n",
    "trainX_tensor = torch.FloatTensor(trainX)\n",
    "trainY_tensor = torch.FloatTensor(trainY)\n",
    "testX_tensor = torch.FloatTensor(testX)\n",
    "testY_tensor = torch.FloatTensor(testY)\n",
    "\n",
    "# 텐서 형태로 데이터 정의\n",
    "dataset = TensorDataset(trainX_tensor, trainY_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de868730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더는 기본적으로 2개의 인자를 입력받으며 배치크기는 통상적으로 2의 배수를 사용\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=True,  \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "466ac432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train loss : 17713042.0000\n",
      "Epoch: 0020 train loss : 17768420.0000\n",
      "\n",
      " Early Stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAFuCAYAAABDbcMuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA970lEQVR4nO3deXhU5d3/8c/MJDMJgUyAQBYJEBbZhIAgMaitXEYDpTxiKyoPFaQuV11aMcUl/SmgaOOCFlEqrYqB1gr6qFiXohgNVA271B1Bo2FJwiJkSIBJMnN+f+AcnBIwA5mZQ/J+XddcZM7cc+YePCofvvf9PTbDMAwBAAAAQCtjj/YEAAAAACAaCEMAAAAAWiXCEAAAAIBWiTAEAAAAoFUiDAEAAABolQhDAAAAAFolwhAAAACAVokwBAAAAKBVIgwBAAAAaJUIQwAAAABapRYXhlauXKmxY8cqPT1dNptNS5cuDen9M2fOlM1mO+qRkJAQngkDAAAAiIoWF4Zqa2uVlZWlefPmndD7p02bpoqKiqBH//79NX78+GaeKQAAAIBoanFhaPTo0br33nt1ySWXNPq61+vVtGnTdNpppykhIUHZ2dkqKSkxX2/btq1SU1PNR1VVlT777DNdffXVEfoGAAAAACKhxYWhH3PTTTeptLRUixcv1kcffaTx48dr1KhR2rx5c6Pjn3rqKZ1++uk677zzIjxTAAAAAOHUqsJQeXm5nnnmGb3wwgs677zz1LNnT02bNk3nnnuunnnmmaPGHzp0SM8++yxVIQAAAKAFion2BCLp448/ls/n0+mnnx503Ov1qmPHjkeNf/nll7V//35Nnjw5UlMEAAAAECGtKgzV1NTI4XBo/fr1cjgcQa+1bdv2qPFPPfWUfv7znyslJSVSUwQAAAAQIa0qDA0ZMkQ+n087d+780T1AZWVlevfdd/XPf/4zQrMDAAAAEEktLgzV1NRoy5Yt5vOysjJt3LhRHTp00Omnn66JEydq0qRJevjhhzVkyBDt2rVLxcXFGjRokMaMGWO+b8GCBUpLS9Po0aOj8TUAAAAAhJnNMAwj2pNoTiUlJRo5cuRRxydPnqyioiLV19fr3nvv1aJFi7R9+3YlJyfr7LPP1t13362BAwdKkvx+v7p166ZJkybpvvvui/RXAAAAABABLS4MAQAAAEBTtKrW2gAAAAAQ0CL2DPn9fu3YsUPt2rWTzWaL9nQAAAAARIlhGNq/f7/S09Nltx+/9tMiwtCOHTuUkZER7WkAAAAAsIitW7eqS5cuxx3TIsJQu3btJB3+womJiVGeDQAAAIBo8Xg8ysjIMDPC8bSIMBRYGpeYmEgYAgAAANCk7TM0UAAAAADQKhGGAAAAALRKhCEAAAAArVKL2DMEAACAlsvv96uuri7a04CFxMbGyuFwnPR5CEMAAACwrLq6OpWVlcnv90d7KrCYpKQkpaamntR9RglDAAAAsCTDMFRRUSGHw6GMjIwfvYEmWgfDMHTgwAHt3LlTkpSWlnbC5yIMAQAAwJIaGhp04MABpaenq02bNtGeDiwkPj5ekrRz50517tz5hJfMEa8BAABgST6fT5LkdDqjPBNYUSAg19fXn/A5CEMAAACwtJPZE4KWqzmuC8IQAAAAgFaJPUMR9Mn2aj1R8pUykxP0m/N7qq2L334AAAAgWqgMRUBF9UHlP79RYx9/T69/XKHH392ikbNL9MK6rfL7jWhPDwAAABbXvXt3zZkzp8njS0pKZLPZtG/fvrDNSZKKioqUlJQU1s8IJ0oTYVTjbdBfVnylJ//9tQ7VH+6NP/qMVH1e4dE3ew7o1v/7SH9b9a1mjB2god3aR3m2p55ab4M+2V6t/2zbp+qD9Tqza3udldlBiXGxYf/svbV1auNyyBVz8jf7AgAALceP7WOZMWOGZs6cGfJ5165dq4SEhCaPHzFihCoqKuR2u0P+rNaEMBQGDT6/Xli/TQ+/9aV213glScO6tdedP++vwRlJ8jb4tPCDbzS3eIs+2latXz7xgcYNTtfto/sqzR0flvnsPVCv72rrtKfGqz3f//pdbZ0O1vvUO6WdBnVxq1entopxWK9Y6PMbOlTvU/l3B/Sfrfu08fvHl1X79d+FNbtNGpDu1tk9OujsHh01rHsHueNPPhxVH6zXqq/36P0tu/Xelt36elet7DYpo0MbZSYnqEdyW2V2SlDP5ARldkpQ53ZxctiP/x/Dep9fW787oLLdteajf3qiJmZ3O+n5AgCA6KioqDB/XrJkiaZPn65NmzaZx9q2bWv+bBiGfD6fYmJ+/I/knTp1CmkeTqdTqampIb2nNSIMNbPSr/Zo5j8/1aaq/ZKkbh3bqGB0X+UNOHJ3XFeMQ9f9pKcuGdJFs9/cpOfXb9XSjTv05qdV+snpyfL5D/9Bud7nV13D97/6DLnjYzSka3sN7dpeZ3Zrrw4JjbeZ3F3j1Zqy77T66z1aXfadNlXtl9GE1XhxsXb1S0vUoNPcOuM0t/qlJcoVY1eD35DPb3z/q18NvsM/1zX45W3wy9vgk7fBbz6va/DrtPbxGpKRpC7t44/7NyR+v6HPKz36YMselX69Rzv2HdTBep8O1ft0sM6nQ/V+1fmOfcfpNHecsrokKTE+Rmu/2auy3bX6eHu1Pt5erSf/XSabTerVqa0SXDFyxtjlirHL6bDLGWM3nye4YtTOFaO2cTFq64pV27jDz202ad03e/Xelt36aNu+o4KX35C+3XNA3+45oJJNu46aW1tXjBLjYpQYH6vEuFglxseoXVysqg/Wq2x3rcq/OyDff53UZpPGZqVHpLoFAMCpxjAMHaz3ReWz42MdTepe9sMA4na7ZbPZzGMlJSUaOXKk3njjDd155536+OOP9dZbbykjI0P5+flatWqVamtr1a9fPxUWFio3N9c8V/fu3TV16lRNnTpV0uEK1JNPPqnXX39db775pk477TQ9/PDD+p//+Z+gz9q7d6+SkpJUVFSkqVOnasmSJZo6daq2bt2qc889V88884x509KGhgbl5+dr0aJFcjgcuuaaa1RZWanq6motXbq0yb9XTzzxhGbPnq2tW7cqMzNTd955p6688kpJh/8Z3n333VqwYIGqqqrUsWNHXXrppZo7d64k6c9//rP+9Kc/aevWrXK73TrvvPP0f//3f03+7FCFHIZWrlyphx56SOvXr1dFRYVefvlljRs37pjjr7rqKi1cuPCo4/3799enn34qSZo5c6buvvvuoNf79OmjL774ItTpRV2l56A2Ve2XOz5Wv7ugt648u5ucMY1XWzq1c+mBSwfpV2d3092vfqp13+7Vm59WHff8q77+zvw5MzlBQ7omaWi39mrrijkcgMq+05adNUe9z2aTkuJj1bGtSx0SnEpu61SHBKdi7HZ9XuHRpzs8qvE26MPyffqwfN9J/R78UMcEpwZnJB1+dE3SoC5J2l3j1QdbduuDrw4HoH0HmtYbPsHp0KAuh8+T1SVJQ7omKSUxLmhMleeQVn29R6u+PhwGv95dq82N/H6ciB6dEnRur2Sd0ytZZ2d2lLfBp692Ha7ofL2rRl9/X90JhJwab4NqvA3aUX3omOeMj3Woe3KCeiQn6I1PKmQY0sE6H2EIAIBGHKz3qf/0N6Py2Z/dk6c2zuapI9xxxx2aPXu2evToofbt22vr1q362c9+pvvuu08ul0uLFi3S2LFjtWnTJnXt2vWY57n77rv14IMP6qGHHtJjjz2miRMn6ttvv1WHDh0aHX/gwAHNnj1bf/vb32S32/WrX/1K06ZN07PPPitJeuCBB/Tss8/qmWeeUb9+/fToo49q6dKlGjlyZJO/28svv6ybb75Zc+bMUW5url577TVNmTJFXbp00ciRI/Xiiy/qT3/6kxYvXqwBAwaosrJS//nPfyRJ69at0+9+9zv97W9/04gRI/Tdd9/p3//+dwi/s6EL+Z9obW2tsrKy9Otf/1q/+MUvfnT8o48+qvvvv9983tDQoKysLI0fPz5o3IABA/T2228fmVgTyoVWdHHWadq136vLhmUoqU3TbhA2sItbL/wmRyWbdmnr3gOKddgVG6heOGzm84rqg9rw7T6tL9+rLTtrzKVVL23YftQ5+6a2U3ZmB2X36KghXZPUqa3ruEvg/H5DZXtq9cn2an207XBlZcvOGhmGIYfdrliHTQ67TTH2w7867Da5YhyHKy3fV1hcMQ45Y+yKsdu0ZVeNPtvh0Z7aOhV/sVPFX+w85mcnOB3K7tFRI3p2VJ/UdmrjPLwXJ97pUHysQ3GxgV/tP/o3MimJcbp48Gm6ePBpkqSdnkP6onK/WbGq8/nk/b7aFKhk1XgbVHPocHDZf6hBNd561XoPV6cGpCfqnO8DUHrSfy9hjFXnxDjl9OwYdLTB55fnUIM8B+vlOVQvz8GG7389/LyNM0Y9OiUoMzlBKe3iZP9+OV3fu/51uBLWcOxKGAAAOPXdc889uvDCC83nHTp0UFZWlvl81qxZevnll/XPf/5TN9100zHPc9VVV2nChAmSpD/+8Y+aO3eu1qxZo1GjRjU6vr6+XvPnz1fPnj0lSTfddJPuuece8/XHHntMBQUFuuSSSyRJjz/+uN54442Qvtvs2bN11VVX6YYbbpAks+I1e/ZsjRw5UuXl5UpNTVVubq5iY2PVtWtXDR8+XJJUXl6uhIQE/fznP1e7du3UrVs3DRkyJKTPD1XIiWP06NEaPXp0k8e73e6gjVtLly7V3r17NWXKlOCJxMS0iHWNdrtN1/2kZ8jvs9lsGtm384+Ou/ysw387UH2gXhu27tWH3+7V+vK9qvH6NLRre2X36KDh3Tuo/TGW0B1v3j07tVXPTm3NIHGyDtX79OkOjzZu3Wfu9Sn/7oCcMXYN7dpe5/TqqJyeyRrUxa3YMO1V6pwYp87/VT0KtxiHXR0SnMdcxngsTof9R5cFAgDQmsXHOvTZPXlR++zmMmzYsKDnNTU1mjlzpl5//XVVVFSooaFBBw8eVHl5+XHPM2jQIPPnhIQEJSYmaufOY/8FdJs2bcwgJElpaWnm+OrqalVVVZnBRJIcDoeGDh0qv7/pfzb5/PPPdd111wUdO+ecc/Too49KksaPH685c+aoR48eGjVqlH72s59p7NixiomJ0YUXXqhu3bqZr40aNUqXXHKJ2rRp0+TPD1XEyy9PP/20cnNz1a1b8CbxzZs3Kz09XXFxccrJyVFhYeExy4Jer1der9d87vF4wjpnK3K3idXIPp01ss+PB6hoiYt1aGi39kGd8qoP1MsVa1dcM/4HpaUILKekMgQAQONsNluzLVWLpv/uCjdt2jQtX75cs2fPVq9evRQfH69LL71UdXV1xz1PbGzwsnqbzXbc4NLYeKMpG8ubUUZGhjZt2qS3335by5cv1w033KCHHnpIK1asULt27bRhwwaVlJTorbfe0vTp0zVz5kytXbs2bO27I9o6bMeOHfrXv/6la665Juh4dna2ioqKtGzZMj3xxBMqKyvTeeedp/379zd6nsLCQrPi5Ha7lZGREYnpoxm428QShI7B6SAMAQDQGr3//vu66qqrdMkll2jgwIFKTU3VN998E9E5uN1upaSkaO3ateYxn8+nDRs2hHSefv366f333w869v7776t///7m8/j4eI0dO1Zz585VSUmJSktL9fHHH0s6vFosNzdXDz74oD766CN98803euedd07imx1fRKP1woULlZSUdFTDhR8uuxs0aJCys7PVrVs3Pf/887r66quPOk9BQYHy8/PN5x6Ph0CEU16gMlTPMjkAAFqV3r1766WXXtLYsWNls9l01113hbQ0rbn89re/VWFhoXr16qW+ffvqscce0969e5vURS/g1ltv1WWXXaYhQ4YoNzdXr776ql566SWzN0BRUZF8Pp+ys7PVpk0b/f3vf1d8fLy6deum1157TV9//bV+8pOfqH379nrjjTfk9/vVp0+fcH3lyIUhwzC0YMECXXnllXI6j7+XIikpSaeffrq2bNnS6Osul0sulysc0wSiJpbKEAAArdIjjzyiX//61xoxYoSSk5N1++23R2UbyO23367KykpNmjRJDodD1113nfLy8uRwNH1Vz7hx4/Too49q9uzZuvnmm5WZmalnnnlG559/vqTDf86///77lZ+fL5/Pp4EDB+rVV19Vx44dlZSUpJdeekkzZ87UoUOH1Lt3bz333HMaMGBAmL6xZDNOYqGgzWb70dbaAYFe5x9//LHOOOOM446tqalR165dNXPmTP3ud7/70XN7PB653W5VV1crMTGxqdMHLGXM3H/r0x0eFU05S+dbeC8YAACRcujQIZWVlSkzM1NxcZFtiATJ7/erX79+uuyyyzRr1qxoT+cox7o+QskGIVeGampqgio2ZWVl2rhxozp06KCuXbuqoKBA27dv16JFi4Le9/TTTys7O7vRIDRt2jSNHTtW3bp1044dOzRjxgw5HA6zVSDQGlAZAgAA0fTtt9/qrbfe0k9/+lN5vV49/vjjKisr0//+7/9Ge2phE3IYWrduXdCNlwJ7dyZPnqyioiJVVFQc1QawurpaL774otlS779t27ZNEyZM0J49e9SpUyede+65WrVqlTp16hTq9IBTltlNjj1DAAAgCux2u4qKijRt2jQZhqEzzjhDb7/9tvr16xftqYVNyGHo/PPPP24LvqKioqOOud1uHThw4JjvWbx4cajTAFqcQDc5GigAAIBoyMjIOKoTXEsX0dbaAI6N+wwBAABEFmEIsAjzPkO+yN78DAAAq4v0jUFxamiO9uOn/i18gRYilsoQAABBYmNjZbPZtGvXLnXq1Cmk+92g5TIMQ3V1ddq1a5fsdvuP3rbneAhDgEWwZwgAgGAOh0NdunTRtm3b9M0330R7OrCYNm3aqGvXrrLbT3yxG2EIsAhnzOG/7aIyBADAEW3btlXv3r1VX18f7anAQhwOh2JiYk66WkgYAizCyX2GAABolMPhkMPhiPY00ALRQAGwiFiWyQEAAEQUYQiwiEBrbS+VIQAAgIggDAEWEQhDVIYAAAAigzAEWEQse4YAAAAiijAEWISLyhAAAEBEEYYAizArQ4QhAACAiCAMARYR2DPEMjkAAIDIIAwBFmHeZ8hnRHkmAAAArQNhCLCIWLMy5IvyTAAAAFoHwhBgEU7zpqtUhgAAACKBMARYhDPGJok9QwAAAJFCGAIswulwSCIMAQAARAphCLCIWMfhyhD3GQIAAIgMwhBgEYHW2l4qQwAAABFBGAIsIhCGqAwBAABEBmEIsIgj9xkiDAEAAEQCYQiwCLMyxDI5AACAiCAMARYRS2UIAAAgoghDgEUc2TNkyO/nxqsAAADhRhgCLCJQGZKkej/VIQAAgHAjDAEW4Yo58q8jN14FAAAIP8IQYBFBlSEfy+QAAADCLeQwtHLlSo0dO1bp6emy2WxaunTpcceXlJTIZrMd9aisrAwaN2/ePHXv3l1xcXHKzs7WmjVrQp0acEpz2G1y2G2SqAwBAABEQshhqLa2VllZWZo3b15I79u0aZMqKirMR+fOnc3XlixZovz8fM2YMUMbNmxQVlaW8vLytHPnzlCnB5zSAvca4sarAAAA4RcT6htGjx6t0aNHh/xBnTt3VlJSUqOvPfLII7r22ms1ZcoUSdL8+fP1+uuva8GCBbrjjjtC/izgVBXrsOlgveSlMgQAABB2EdszNHjwYKWlpenCCy/U+++/bx6vq6vT+vXrlZube2RSdrtyc3NVWlra6Lm8Xq88Hk/QA2gJnDEOSSyTAwAAiISwh6G0tDTNnz9fL774ol588UVlZGTo/PPP14YNGyRJu3fvls/nU0pKStD7UlJSjtpXFFBYWCi3220+MjIywv01gIhwxbBMDgAAIFJCXiYXqj59+qhPnz7m8xEjRuirr77Sn/70J/3tb387oXMWFBQoPz/ffO7xeAhEaBFiHd83UCAMAQAAhF3Yw1Bjhg8frvfee0+SlJycLIfDoaqqqqAxVVVVSk1NbfT9LpdLLpcr7PMEIs0ZqAyxTA4AACDsonKfoY0bNyotLU2S5HQ6NXToUBUXF5uv+/1+FRcXKycnJxrTA6ImcK8hL5UhAACAsAu5MlRTU6MtW7aYz8vKyrRx40Z16NBBXbt2VUFBgbZv365FixZJkubMmaPMzEwNGDBAhw4d0lNPPaV33nlHb731lnmO/Px8TZ48WcOGDdPw4cM1Z84c1dbWmt3lgNYiUBmigQIAAED4hRyG1q1bp5EjR5rPA3t3Jk+erKKiIlVUVKi8vNx8va6uTr///e+1fft2tWnTRoMGDdLbb78ddI7LL79cu3bt0vTp01VZWanBgwdr2bJlRzVVAFq6WO4zBAAAEDE2wzCMaE/iZHk8HrndblVXVysxMTHa0wFO2JVPr9a/N+/WI5dl6Rdndon2dAAAAE45oWSDqOwZAtA4J5UhAACAiCEMARYSWCbHniEAAIDwIwwBFmI2UPCd8qtXAQAALI8wBFgIlSEAAIDIIQwBFkJrbQAAgMghDAEW4nTYJNFAAQAAIBIIQ4CFHNkzRBgCAAAIN8IQYCEskwMAAIgcwhBgIWYDBSpDAAAAYUcYAiwkUBmqpzIEAAAQdoQhwEKcVIYAAAAihjAEWAh7hgAAACKHMARYSKAyRGttAACA8CMMARYSaKDgpTIEAAAQdoQhwELMBgpUhgAAAMKOMARYiNlam8oQAABA2BGGAAtxxdBNDgAAIFIIQ4CFBCpD9Q1GlGcCAADQ8hGGAAtxUhkCAACIGMIQYCHcZwgAACByCEOAhcQ6bJKoDAEAAEQCYQiwEBettQEAACKGMARYCK21AQAAIocwBFgIe4YAAAAihzAEWEigMtTgN+T3014bAAAgnAhDgIUEKkMSTRQAAADCjTAEWIjTceRfSZooAAAAhBdhCLCQH4Yh9g0BAACEV8hhaOXKlRo7dqzS09Nls9m0dOnS445/6aWXdOGFF6pTp05KTExUTk6O3nzzzaAxM2fOlM1mC3r07ds31KkBpzy73aYY++F7DdX72DMEAAAQTiGHodraWmVlZWnevHlNGr9y5UpdeOGFeuONN7R+/XqNHDlSY8eO1Ycffhg0bsCAAaqoqDAf7733XqhTA1oE2msDAABERkyobxg9erRGjx7d5PFz5swJev7HP/5Rr7zyil599VUNGTLkyERiYpSamtqkc3q9Xnm9XvO5x+Np8nwAq3PG2HWw3qc6ny/aUwEAAGjRIr5nyO/3a//+/erQoUPQ8c2bNys9PV09evTQxIkTVV5efsxzFBYWyu12m4+MjIxwTxuImCP3GmKZHAAAQDhFPAzNnj1bNTU1uuyyy8xj2dnZKioq0rJly/TEE0+orKxM5513nvbv39/oOQoKClRdXW0+tm7dGqnpA2EXaKJAa20AAIDwCnmZ3Mn4xz/+obvvvluvvPKKOnfubB7/4bK7QYMGKTs7W926ddPzzz+vq6+++qjzuFwuuVyuiMwZiLRAZYjW2gAAAOEVsTC0ePFiXXPNNXrhhReUm5t73LFJSUk6/fTTtWXLlgjNDrCOWMfhbnI0UAAAAAiviCyTe+655zRlyhQ999xzGjNmzI+Or6mp0VdffaW0tLQIzA6wliN7hghDAAAA4RRyZaimpiaoYlNWVqaNGzeqQ4cO6tq1qwoKCrR9+3YtWrRI0uGlcZMnT9ajjz6q7OxsVVZWSpLi4+PldrslSdOmTdPYsWPVrVs37dixQzNmzJDD4dCECROa4zsCp5RY9gwBAABERMiVoXXr1mnIkCFmW+z8/HwNGTJE06dPlyRVVFQEdYL761//qoaGBt14441KS0szHzfffLM5Ztu2bZowYYL69Omjyy67TB07dtSqVavUqVOnk/1+wCnHyX2GAAAAIiLkytD5558vwzh2y9+ioqKg5yUlJT96zsWLF4c6DaDFooECAABAZES8tTaA46MyBAAAEBmEIcBiqAwBAABEBmEIsJhAAwUvlSEAAICwIgwBFmO21qYyBAAAEFaEIcBiApWh+oZjNyoBAADAySMMARbjMitDvijPBAAAoGUjDAEWc6SBApUhAACAcCIMARYT67BJorU2AABAuBGGAItxOhySaKAAAAAQboQhwGJiY6gMAQAARAJhCLAY5/fd5AhDAAAA4UUYAizGZTZQIAwBAACEE2EIsJhYKkMAAAARQRgCLMZp3meIMAQAABBOhCHAYqgMAQAARAZhCLAYKkMAAACRQRgCLCbQTY4GCgAAAOFFGAIsxqwMsUwOAAAgrAhDgMU4zdbaRpRnAgAA0LIRhgCLoYECAABAZBCGAIsJ7BmigQIAAEB4EYYAi3HG2CRRGQIAAAg3whBgMU6HQxJhCAAAINwIQ4DFxH5fGaK1NgAAQHgRhgCLCewZavAb8vvpKAcAABAuhCHAYgKttSWaKAAAAIQTYQiwmEBrbYkwBAAAEE6EIcBinD8IQ/U0UQAAAAibkMPQypUrNXbsWKWnp8tms2np0qU/+p6SkhKdeeaZcrlc6tWrl4qKio4aM2/ePHXv3l1xcXHKzs7WmjVrQp0a0CLY7TbF2L9vr01lCAAAIGxCDkO1tbXKysrSvHnzmjS+rKxMY8aM0ciRI7Vx40ZNnTpV11xzjd58801zzJIlS5Sfn68ZM2Zow4YNysrKUl5ennbu3Bnq9IAWIbBviPbaAAAA4WMzDOOE21XZbDa9/PLLGjdu3DHH3H777Xr99df1ySefmMeuuOIK7du3T8uWLZMkZWdn66yzztLjjz8uSfL7/crIyNBvf/tb3XHHHUed0+v1yuv1ms89Ho8yMjJUXV2txMTEE/06gGUMvuct7TtQr7fzf6JendtFezoAAACnDI/HI7fb3aRsEPY9Q6WlpcrNzQ06lpeXp9LSUklSXV2d1q9fHzTGbrcrNzfXHPPfCgsL5Xa7zUdGRkb4vgAQBYEmCl4qQwAAAGET9jBUWVmplJSUoGMpKSnyeDw6ePCgdu/eLZ/P1+iYysrKRs9ZUFCg6upq87F169awzR+IhkAThXof9xkCAAAIl5hoT+BEuFwuuVyuaE8DCBv2DAEAAIRf2MNQamqqqqqqgo5VVVUpMTFR8fHxcjgccjgcjY5JTU0N9/QASwpUhghDAAAA4RP2ZXI5OTkqLi4OOrZ8+XLl5ORIkpxOp4YOHRo0xu/3q7i42BwDtDaxMYdba9fTWhsAACBsQg5DNTU12rhxozZu3CjpcOvsjRs3qry8XNLh/TyTJk0yx//mN7/R119/rdtuu01ffPGF/vznP+v555/XLbfcYo7Jz8/Xk08+qYULF+rzzz/X9ddfr9raWk2ZMuUkvx5wanLSQAEAACDsQl4mt27dOo0cOdJ8np+fL0maPHmyioqKVFFRYQYjScrMzNTrr7+uW265RY8++qi6dOmip556Snl5eeaYyy+/XLt27dL06dNVWVmpwYMHa9myZUc1VQBai8CeISpDAAAA4XNS9xmyilB6iQOngiufXq1/b96th8dn6ZdDu0R7OgAAAKcMS91nCEDoXFSGAAAAwo4wBFhQ4KardYQhAACAsCEMARbEfYYAAADCjzAEWJCTyhAAAEDYEYYAC4qlMgQAABB2hCHAggKVIRooAAAAhA9hCLAg9gwBAACEH2EIsKAjlaFT/jZgAAAAlkUYAiwo0FrbS2UIAAAgbAhDgAWxTA4AACD8CEOABQXCEA0UAAAAwocwBFiQ02GTRGUIAAAgnAhDgAVRGQIAAAg/whBgQYEGCnWEIQAAgLAhDAEWFKgM0U0OAAAgfAhDgAXFOlgmBwAAEG6EIcCCaK0NAAAQfoQhwIJcVIYAAADCjjAEWFAslSEAAICwIwwBFuQ0K0NGlGcCAADQchGGAAsKNFCgmxwAAED4EIYACzrSQMEX5ZkAAAC0XIQhwIJcMSyTAwAACDfCEGBBgWVydXSTAwAACBvCEGBBgWVyPr8hn5/qEAAAQDgQhgALinXYzJ+51xAAAEB4EIYACwpUhiSWygEAAIQLYQiwoFj7D8IQ7bUBAADC4oTC0Lx589S9e3fFxcUpOztba9asOebY888/Xzab7ajHmDFjzDFXXXXVUa+PGjXqRKYGtAh2u81cKkcYAgAACI+YUN+wZMkS5efna/78+crOztacOXOUl5enTZs2qXPnzkeNf+mll1RXV2c+37Nnj7KysjR+/PigcaNGjdIzzzxjPne5XKFODWhRnA676n0+9gwBAACESchh6JFHHtG1116rKVOmSJLmz5+v119/XQsWLNAdd9xx1PgOHToEPV+8eLHatGlzVBhyuVxKTU1t0hy8Xq+8Xq/53OPxhPo1AMuLjbFLdT4qQwAAAGES0jK5uro6rV+/Xrm5uUdOYLcrNzdXpaWlTTrH008/rSuuuEIJCQlBx0tKStS5c2f16dNH119/vfbs2XPMcxQWFsrtdpuPjIyMUL4GcEpwcq8hAACAsAopDO3evVs+n08pKSlBx1NSUlRZWfmj71+zZo0++eQTXXPNNUHHR40apUWLFqm4uFgPPPCAVqxYodGjR8vn8zV6noKCAlVXV5uPrVu3hvI1gFOCeeNVKkMAAABhEfIyuZPx9NNPa+DAgRo+fHjQ8SuuuML8eeDAgRo0aJB69uypkpISXXDBBUedx+VysacILZ4rhjAEAAAQTiFVhpKTk+VwOFRVVRV0vKqq6kf3+9TW1mrx4sW6+uqrf/RzevTooeTkZG3ZsiWU6QEtSqAyVO8zojwTAACAlimkMOR0OjV06FAVFxebx/x+v4qLi5WTk3Pc977wwgvyer361a9+9aOfs23bNu3Zs0dpaWmhTA9oUQI3Xq07xnJRAAAAnJyQ7zOUn5+vJ598UgsXLtTnn3+u66+/XrW1tWZ3uUmTJqmgoOCo9z399NMaN26cOnbsGHS8pqZGt956q1atWqVvvvlGxcXFuvjii9WrVy/l5eWd4NcCTn1mGGqgMgQAABAOIe8Zuvzyy7Vr1y5Nnz5dlZWVGjx4sJYtW2Y2VSgvL5fdHpyxNm3apPfee09vvfXWUedzOBz66KOPtHDhQu3bt0/p6em66KKLNGvWLPYFoVUzb7pKNzkAAICwsBmGccr/tbPH45Hb7VZ1dbUSExOjPR2gWUxasEYrv9ylh8dn6ZdDu0R7OgAAAKeEULJByMvkAESGk8oQAABAWBGGAIty0lobAAAgrAhDgEU5zdbahCEAAIBwIAwBFhW4z5CXyhAAAEBYEIYAiwosk6MyBAAAEB6EIcCiApUh9gwBAACEB2EIsCgXlSEAAICwIgwBFkVlCAAAILwIQ4BFma21qQwBAACEBWEIsKgj9xkyojwTAACAlokwBFiUuUyOyhAAAEBYEIYAizJba7NnCAAAICwIQ4BFOR02SVSGAAAAwoUwBFjUkT1DhCEAAIBwIAwBFsWeIQAAgPAiDAEW5eQ+QwAAAGFFGAIsymygQGUIAAAgLAhDgEVRGQIAAAgvwhBgUVSGAAAAwoswBFhULJUhAACAsCIMARZlttamMgQAABAWhCHAorjPEAAAQHgRhgCLcnKfIQAAgLAiDAEWdaSBghHlmQAAALRMhCHAogINFHx+Qz4/gQgAAKC5EYYAiwpUhiTaawMAAIQDYQiwqFiHzfzZSxMFAACAZkcYAiwq0EBBoqMcAABAOJxQGJo3b566d++uuLg4ZWdna82aNcccW1RUJJvNFvSIi4sLGmMYhqZPn660tDTFx8crNzdXmzdvPpGpAS2GzWYzAxHL5AAAAJpfyGFoyZIlys/P14wZM7RhwwZlZWUpLy9PO3fuPOZ7EhMTVVFRYT6+/fbboNcffPBBzZ07V/Pnz9fq1auVkJCgvLw8HTp0KPRvBLQggaVyVIYAAACaX8hh6JFHHtG1116rKVOmqH///po/f77atGmjBQsWHPM9NptNqamp5iMlJcV8zTAMzZkzR3feeacuvvhiDRo0SIsWLdKOHTu0dOnSRs/n9Xrl8XiCHkBLdKS9NmEIAACguYUUhurq6rR+/Xrl5uYeOYHdrtzcXJWWlh7zfTU1NerWrZsyMjJ08cUX69NPPzVfKysrU2VlZdA53W63srOzj3nOwsJCud1u85GRkRHK1wBOGYH22jRQAAAAaH4hhaHdu3fL5/MFVXYkKSUlRZWVlY2+p0+fPlqwYIFeeeUV/f3vf5ff79eIESO0bds2STLfF8o5CwoKVF1dbT62bt0aytcAThmBylAdlSEAAIBmFxPuD8jJyVFOTo75fMSIEerXr5/+8pe/aNasWSd0TpfLJZfL1VxTBCzLbKBAZQgAAKDZhVQZSk5OlsPhUFVVVdDxqqoqpaamNukcsbGxGjJkiLZs2SJJ5vtO5pxAS0VlCAAAIHxCCkNOp1NDhw5VcXGxeczv96u4uDio+nM8Pp9PH3/8sdLS0iRJmZmZSk1NDTqnx+PR6tWrm3xOoKWigQIAAED4hLxMLj8/X5MnT9awYcM0fPhwzZkzR7W1tZoyZYokadKkSTrttNNUWFgoSbrnnnt09tlnq1evXtq3b58eeughffvtt7rmmmskHe40N3XqVN17773q3bu3MjMzdddddyk9PV3jxo1rvm8KnIICDRRorQ0AAND8Qg5Dl19+uXbt2qXp06ersrJSgwcP1rJly8wGCOXl5bLbjxSc9u7dq2uvvVaVlZVq3769hg4dqg8++ED9+/c3x9x2222qra3Vddddp3379uncc8/VsmXLjro5K9DaBPYM1fmMKM8EAACg5bEZhnHK/ynL4/HI7XarurpaiYmJ0Z4O0GwmLVijlV/u0uzxWbp0aJdoTwcAAMDyQskGId90FUDkOFkmBwAAEDaEIcDCXDRQAAAACBvCEGBhsQ6bJCpDAAAA4UAYAiyM+wwBAACED2EIsDBaawMAAIQPYQiwMG66CgAAED6EIcDC6CYHAAAQPoQhwMLYMwQAABA+hCHAwgKVIZbJAQAAND/CEGBhsd9XhrwskwMAAGh2hCHAwo5UhowozwQAAKDlIQwBFhaoDNU1+KI8EwAAgJaHMARYmItucgAAAGFDGAIsLDbGJollcgAAAOFAGAIszOlwSKIyBAAAEA6EIcDCuM8QAABA+BCGAAuLdRxeJkdlCAAAoPkRhgALC1SGuOkqAABA8yMMARYWuM8Qy+QAAACaH2EIsDBzzxDL5AAAAJodYQiwMJbJAQAAhA9hCLCw2O+XyXmpDAEAADQ7whBgYYE9Q1SGAAAAmh9hCLAw9gwBAACED2EIsLBAZchvSD6/EeXZAAAAtCyEIcDCYmOO/CtKdQgAAKB5EYYACwtUhiTCEAAAQHMjDAEWFuuwmT9z41UAAIDmdUJhaN68eerevbvi4uKUnZ2tNWvWHHPsk08+qfPOO0/t27dX+/btlZube9T4q666SjabLegxatSoE5ka0KLYbDazOkQYAgAAaF4hh6ElS5YoPz9fM2bM0IYNG5SVlaW8vDzt3Lmz0fElJSWaMGGC3n33XZWWliojI0MXXXSRtm/fHjRu1KhRqqioMB/PPffciX0joIUxb7zKMjkAAIBmFXIYeuSRR3TttddqypQp6t+/v+bPn682bdpowYIFjY5/9tlndcMNN2jw4MHq27evnnrqKfn9fhUXFweNc7lcSk1NNR/t27c/5hy8Xq88Hk/QA2ipAkvlqAwBAAA0r5DCUF1dndavX6/c3NwjJ7DblZubq9LS0iad48CBA6qvr1eHDh2CjpeUlKhz587q06ePrr/+eu3Zs+eY5ygsLJTb7TYfGRkZoXwN4JTCvYYAAADCI6QwtHv3bvl8PqWkpAQdT0lJUWVlZZPOcfvttys9PT0oUI0aNUqLFi1ScXGxHnjgAa1YsUKjR4+Wz+dr9BwFBQWqrq42H1u3bg3lawCnlFj2DAEAAIRFTCQ/7P7779fixYtVUlKiuLg48/gVV1xh/jxw4EANGjRIPXv2VElJiS644IKjzuNyueRyuSIyZyDaqAwBAACER0iVoeTkZDkcDlVVVQUdr6qqUmpq6nHfO3v2bN1///166623NGjQoOOO7dGjh5KTk7Vly5ZQpge0SIFucvVUhgAAAJpVSGHI6XRq6NChQc0PAs0QcnJyjvm+Bx98ULNmzdKyZcs0bNiwH/2cbdu2ac+ePUpLSwtlekCLRGUIAAAgPELuJpefn68nn3xSCxcu1Oeff67rr79etbW1mjJliiRp0qRJKigoMMc/8MADuuuuu7RgwQJ1795dlZWVqqysVE1NjSSppqZGt956q1atWqVvvvlGxcXFuvjii9WrVy/l5eU109cETl1UhgAAAMIj5D1Dl19+uXbt2qXp06ersrJSgwcP1rJly8ymCuXl5bLbj2SsJ554QnV1dbr00kuDzjNjxgzNnDlTDodDH330kRYuXKh9+/YpPT1dF110kWbNmsW+IEBHGih4qQwBAAA0K5thGEa0J3GyPB6P3G63qqurlZiYGO3pAM1q8oI1WvHlLj106SCNH0YbeQAAgOMJJRuEvEwOQGQF9gzV+075v7cAAACwFMIQYHGBPUN1DY3fdwsAAAAnhjAEWByVIQAAgPAgDAEWF+uwSZLq6CYHAADQrAhDgMVxnyEAAIDwIAwBFhdorU1lCAAAoHkRhgCLozIEAAAQHoQhwOJcjkADBcIQAABAcyIMARZnLpOjMgQAANCsCEOAxZnL5KgMAQAANCvCEGBxVIYAAADCgzAEWBwNFAAAAMKDMARYnJMGCgAAAGFBGAIsjj1DAAAA4UEYAiwuEIbqG4wozwQAAKBlIQwBFhdooOClMgQAANCsCEOAxR2pDBGGAAAAmhNhCLC4WIdNEnuGAAAAmhthCLA4F621AQAAwoIwBFic0+GQRGttAACA5kYYAiwuNub7ZXJUhgAAAJoVYQiwuMBNV9kzBAAA0LwIQ4DFBVprUxkCAABoXoQhwOICDRTYMwQAANC8CEOAxQUqQ35DaiAQAQAANBvCEGBxgZuuSuwbAgAAaE6EIcDifhiG6huMKM4EAACgZSEMARYXY7eZP3t9vijOBAAAoGU5oTA0b948de/eXXFxccrOztaaNWuOO/6FF15Q3759FRcXp4EDB+qNN94Iet0wDE2fPl1paWmKj49Xbm6uNm/efCJTA1ocm81mVofqfVSGAAAAmkvIYWjJkiXKz8/XjBkztGHDBmVlZSkvL087d+5sdPwHH3ygCRMm6Oqrr9aHH36ocePGady4cfrkk0/MMQ8++KDmzp2r+fPna/Xq1UpISFBeXp4OHTp04t8MaEGctNcGAABodjbDMEL6q+bs7GydddZZevzxxyVJfr9fGRkZ+u1vf6s77rjjqPGXX365amtr9dprr5nHzj77bA0ePFjz58+XYRhKT0/X73//e02bNk2SVF1drZSUFBUVFemKK6446pxer1der9d87vF4lJGRoerqaiUmJobydYBTwpmzluu72joNSE9UfKwj2tMBAABo1B/G9NOZXdtHdQ4ej0dut7tJ2SAmlBPX1dVp/fr1KigoMI/Z7Xbl5uaqtLS00feUlpYqPz8/6FheXp6WLl0qSSorK1NlZaVyc3PN191ut7Kzs1VaWtpoGCosLNTdd98dytSBU1qX9vH6rrZOn+7wRHsqAAAAx1R9sD7aUwhJSGFo9+7d8vl8SklJCTqekpKiL774otH3VFZWNjq+srLSfD1w7Fhj/ltBQUFQwApUhoCW6pmrztLab/ZKYs8QAACwrgHpp9YqrZDCkFW4XC65XK5oTwOImI5tXRp1Rmq0pwEAANCihNRAITk5WQ6HQ1VVVUHHq6qqlJra+B/UUlNTjzs+8Gso5wQAAACAkxVSGHI6nRo6dKiKi4vNY36/X8XFxcrJyWn0PTk5OUHjJWn58uXm+MzMTKWmpgaN8Xg8Wr169THPCQAAAAAnK+Rlcvn5+Zo8ebKGDRum4cOHa86cOaqtrdWUKVMkSZMmTdJpp52mwsJCSdLNN9+sn/70p3r44Yc1ZswYLV68WOvWrdNf//pXSYfvoTJ16lTde++96t27tzIzM3XXXXcpPT1d48aNa75vCgAAAAA/EHIYuvzyy7Vr1y5Nnz5dlZWVGjx4sJYtW2Y2QCgvL5fdfqTgNGLECP3jH//QnXfeqT/84Q/q3bu3li5dqjPOOMMcc9ttt6m2tlbXXXed9u3bp3PPPVfLli1TXFxcM3xFAAAAADhayPcZsqJQeokDAAAAaLlCyQYh7RkCAAAAgJaCMAQAAACgVSIMAQAAAGiVCEMAAAAAWiXCEAAAAIBWKeTW2lYUaIjn8XiiPBMAAAAA0RTIBE1pmt0iwtD+/fslSRkZGVGeCQAAAAAr2L9/v9xu93HHtIj7DPn9fu3YsUPt2rWTzWaL9nTk8XiUkZGhrVu3ct8jNBnXDU4E1w1OFNcOTgTXDU5EpK8bwzC0f/9+paeny24//q6gFlEZstvt6tKlS7SncZTExET+Q4GQcd3gRHDd4ERx7eBEcN3gRETyuvmxilAADRQAAAAAtEqEIQAAAACtEmEoDFwul2bMmCGXyxXtqeAUwnWDE8F1gxPFtYMTwXWDE2Hl66ZFNFAAAAAAgFBRGQIAAADQKhGGAAAAALRKhCEAAAAArRJhCAAAAECrRBgCAAAA0CoRhprZvHnz1L17d8XFxSk7O1tr1qyJ9pRgIYWFhTrrrLPUrl07de7cWePGjdOmTZuCxhw6dEg33nijOnbsqLZt2+qXv/ylqqqqojRjWNH9998vm82mqVOnmse4bnAs27dv169+9St17NhR8fHxGjhwoNatW2e+bhiGpk+frrS0NMXHxys3N1ebN2+O4owRbT6fT3fddZcyMzMVHx+vnj17atasWfphA2KuG0jSypUrNXbsWKWnp8tms2np0qVBrzflOvnuu+80ceJEJSYmKikpSVdffbVqamoi9h0IQ81oyZIlys/P14wZM7RhwwZlZWUpLy9PO3fujPbUYBErVqzQjTfeqFWrVmn58uWqr6/XRRddpNraWnPMLbfcoldffVUvvPCCVqxYoR07dugXv/hFFGcNK1m7dq3+8pe/aNCgQUHHuW7QmL179+qcc85RbGys/vWvf+mzzz7Tww8/rPbt25tjHnzwQc2dO1fz58/X6tWrlZCQoLy8PB06dCiKM0c0PfDAA3riiSf0+OOP6/PPP9cDDzygBx98UI899pg5husGklRbW6usrCzNmzev0debcp1MnDhRn376qZYvX67XXntNK1eu1HXXXRepryAZaDbDhw83brzxRvO5z+cz0tPTjcLCwijOCla2c+dOQ5KxYsUKwzAMY9++fUZsbKzxwgsvmGM+//xzQ5JRWloarWnCIvbv32/07t3bWL58ufHTn/7UuPnmmw3D4LrBsd1+++3Gueeee8zX/X6/kZqaajz00EPmsX379hkul8t47rnnIjFFWNCYMWOMX//610HHfvGLXxgTJ040DIPrBo2TZLz88svm86ZcJ5999pkhyVi7dq055l//+pdhs9mM7du3R2TeVIaaSV1dndavX6/c3FzzmN1uV25urkpLS6M4M1hZdXW1JKlDhw6SpPXr16u+vj7oOurbt6+6du3KdQTdeOONGjNmTND1IXHd4Nj++c9/atiwYRo/frw6d+6sIUOG6MknnzRfLysrU2VlZdC143a7lZ2dzbXTio0YMULFxcX68ssvJUn/+c9/9N5772n06NGSuG7QNE25TkpLS5WUlKRhw4aZY3Jzc2W327V69eqIzDMmIp/SCuzevVs+n08pKSlBx1NSUvTFF19EaVawMr/fr6lTp+qcc87RGWecIUmqrKyU0+lUUlJS0NiUlBRVVlZGYZawisWLF2vDhg1au3btUa9x3eBYvv76az3xxBPKz8/XH/7wB61du1a/+93v5HQ6NXnyZPP6aOz/XVw7rdcdd9whj8ejvn37yuFwyOfz6b777tPEiRMliesGTdKU66SyslKdO3cOej0mJkYdOnSI2LVEGAKi5MYbb9Qnn3yi9957L9pTgcVt3bpVN998s5YvX664uLhoTwenEL/fr2HDhumPf/yjJGnIkCH65JNPNH/+fE2ePDnKs4NVPf/883r22Wf1j3/8QwMGDNDGjRs1depUpaenc92gxWGZXDNJTk6Ww+E4qntTVVWVUlNTozQrWNVNN92k1157Te+++666dOliHk9NTVVdXZ327dsXNJ7rqHVbv369du7cqTPPPFMxMTGKiYnRihUrNHfuXMXExCglJYXrBo1KS0tT//79g47169dP5eXlkmReH/y/Cz9066236o477tAVV1yhgQMH6sorr9Qtt9yiwsJCSVw3aJqmXCepqalHNRpraGjQd999F7FriTDUTJxOp4YOHari4mLzmN/vV3FxsXJycqI4M1iJYRi66aab9PLLL+udd95RZmZm0OtDhw5VbGxs0HW0adMmlZeXcx21YhdccIE+/vhjbdy40XwMGzZMEydONH/mukFjzjnnnKPa93/55Zfq1q2bJCkzM1OpqalB147H49Hq1au5dlqxAwcOyG4P/iOiw+GQ3++XxHWDpmnKdZKTk6N9+/Zp/fr15ph33nlHfr9f2dnZkZloRNo0tBKLFy82XC6XUVRUZHz22WfGddddZyQlJRmVlZXRnhos4vrrrzfcbrdRUlJiVFRUmI8DBw6YY37zm98YXbt2Nd555x1j3bp1Rk5OjpGTkxPFWcOKfthNzjC4btC4NWvWGDExMcZ9991nbN682Xj22WeNNm3aGH//+9/NMffff7+RlJRkvPLKK8ZHH31kXHzxxUZmZqZx8ODBKM4c0TR58mTjtNNOM1577TWjrKzMeOmll4zk5GTjtttuM8dw3cAwDnc5/fDDD40PP/zQkGQ88sgjxocffmh8++23hmE07ToZNWqUMWTIEGP16tXGe++9Z/Tu3duYMGFCxL4DYaiZPfbYY0bXrl0Np9NpDB8+3Fi1alW0pwQLkdTo45lnnjHHHDx40LjhhhuM9u3bG23atDEuueQSo6KiInqThiX9dxjiusGxvPrqq8YZZ5xhuFwuo2/fvsZf//rXoNf9fr9x1113GSkpKYbL5TIuuOACY9OmTVGaLazA4/EYN998s9G1a1cjLi7O6NGjh/H//t//M7xerzmG6waGYRjvvvtuo3+umTx5smEYTbtO9uzZY0yYMMFo27atkZiYaEyZMsXYv39/xL6DzTB+cDthAAAAAGgl2DMEAAAAoFUiDAEAAABolQhDAAAAAFolwhAAAACAVokwBAAAAKBVIgwBAAAAaJUIQwAAAABaJcIQAAAAgFaJMAQAAACgVSIMAQAAAGiVCEMAAAAAWqX/Dy9xVYTogySeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15944\\379159924.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;31m#(432, 4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m#(188, 5, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mpred_inverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[0mtestY_inverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestY_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred_inverse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jm\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         X = check_array(\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         )\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jm\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\jm\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    # # 기본변수, layer를 초기화해주는 생성자\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, output_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=layers,\n",
    "                            # dropout = 0.1,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim, bias = True) \n",
    "        \n",
    "    # 학습 초기화를 위한 함수\n",
    "    def reset_hidden_state(self): \n",
    "        self.hidden = (\n",
    "                torch.zeros(self.layers, self.seq_len, self.hidden_dim),\n",
    "                torch.zeros(self.layers, self.seq_len, self.hidden_dim))\n",
    "    \n",
    "    # 예측을 위한 함수\n",
    "    def forward(self, x):\n",
    "        x, _status = self.lstm(x)\n",
    "        x = self.fc(x[:, -1])\n",
    "        #print(\"x.shape\",x.shape)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_df, num_epochs = None, lr = None, verbose = 10, patience = 10):\n",
    "     \n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    nb_epochs = num_epochs\n",
    "    \n",
    "    # epoch마다 loss 저장\n",
    "    train_hist = np.zeros(nb_epochs)\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = len(train_df)\n",
    "        \n",
    "        for batch_idx, samples in enumerate(train_df):\n",
    "\n",
    "            x_train, y_train = samples\n",
    "            x_train=x_train.to(device)\n",
    "            y_train=y_train.to(device)\n",
    "            # seq별 hidden state reset\n",
    "            model.reset_hidden_state()\n",
    "            \n",
    "            # H(x) 계산\n",
    "            outputs = model(x_train)\n",
    "                \n",
    "            # cost 계산\n",
    "            loss = criterion(outputs, y_train)                    \n",
    "            \n",
    "            # cost로 H(x) 개선\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_cost += loss/total_batch\n",
    "               \n",
    "        train_hist[epoch] = avg_cost        \n",
    "        \n",
    "        if epoch % verbose == 0:\n",
    "            print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n",
    "            \n",
    "        # patience번째 마다 early stopping 여부 확인\n",
    "        if (epoch % patience == 0) & (epoch != 0):\n",
    "            \n",
    "            # loss가 커졌다면 early stop\n",
    "            if train_hist[epoch-patience] < train_hist[epoch]:\n",
    "                print('\\n Early Stopping')\n",
    "                \n",
    "                break\n",
    "            \n",
    "    return model.eval(), train_hist\n",
    "\n",
    "# 모델 학습\n",
    "net = Net(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)  \n",
    "model, train_hist = train_model(net, dataloader, num_epochs = nb_epochs, lr = learning_rate, verbose = 20, patience = 10)\n",
    "\n",
    "# epoch별 손실값\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 저장    \n",
    "PATH = \"./Timeseries_LSTM_data-02-stock_daily_.pth\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "model = Net(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)  \n",
    "model.load_state_dict(torch.load(PATH), strict=False)\n",
    "model.eval()\n",
    "\n",
    "# 예측 테스트\n",
    "with torch.no_grad(): \n",
    "    pred = []\n",
    "    for pr in range(len(testX_tensor)):\n",
    "\n",
    "        model.reset_hidden_state()\n",
    "\n",
    "        predicted = model(torch.unsqueeze(testX_tensor[pr], 0).to(device) ) \n",
    "        #predicted = torch.flatten(predicted).item()\n",
    "        pred.append(predicted)\n",
    "\n",
    "    # INVERSE\n",
    "    #(432, 4)\n",
    "    #(188, 5, 1)\n",
    "    pred_inverse = scaler_y.inverse_transform(np.array(pred).reshape(-1,1))\n",
    "    testY_inverse = scaler_y.inverse_transform(testY_tensor.reshape(-1,1))\n",
    "    print(\"\",pred_inverse)\n",
    "\n",
    "def MAE(true, pred):\n",
    "    return np.mean(np.abs(true-pred))\n",
    "\n",
    "print('MAE SCORE : ', MAE(pred_inverse, testY_inverse))\n",
    "\n",
    "#MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"MAE\",mean_absolute_error(testY_inverse,pred_inverse))\n",
    "\n",
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE\",mean_squared_error(testY_inverse,pred_inverse))\n",
    "\n",
    "#RMSE\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MSE = mean_squared_error(testY_inverse,pred_inverse)\n",
    "print(\"MSE\",np.sqrt(MSE))\n",
    "# sklearn 은 mse만 제공하기 때문에 rmse는 직접 만들어 써야한다.\n",
    "\n",
    "#MAPE\n",
    "import numpy as np\n",
    "\n",
    "def MAPE(y_test, y_pred):\n",
    "\treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    \n",
    "print(\"MAPE\",MAPE(testY_inverse, pred_inverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1631d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1baab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "testY_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(np.arange(len(pred_inverse)), pred_inverse, label = 'pred')\n",
    "plt.plot(np.arange(len(testY_inverse)), testY_inverse, label = 'true')\n",
    "plt.title(\"Loss plot\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240991d",
   "metadata": {},
   "source": [
    "### 도매가격+감성점수-> 도매가격"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbedad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df_all=pd.read_csv('../data_v3/감성점수와 일별뉴스 합친거.csv',encoding='utf-8')\n",
    "df = df_all.iloc[:,[1,2,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8621fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df[['day_sentiment','경락단가_평균']]\n",
    "df=df[['day_sentiment','경락단가_평균']]\n",
    "\n",
    "\n",
    "# 7일간의 데이터가 입력으로 들어가고 batch size는 임의로 지정\n",
    "seq_length = 5\n",
    "batch = 100\n",
    "\n",
    "# 데이터를 역순으로 정렬하여 전체 데이터의 70% 학습, 30% 테스트에 사용\n",
    "train_size = int(len(df)*0.7)\n",
    "train_set = df[0:train_size]  \n",
    "test_set = df[train_size-seq_length:]\n",
    "\n",
    "# Input scale\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(train_set.iloc[:, :-1])\n",
    "\n",
    "train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n",
    "test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:, :-1])\n",
    "\n",
    "# Output scale\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(train_set.iloc[:, [-1]])\n",
    "\n",
    "train_set.iloc[:, -1:] = scaler_y.transform(train_set.iloc[:, -1:])\n",
    "test_set.iloc[:, -1:] = scaler_y.transform(test_set.iloc[:, -1:])\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series)-seq_length):\n",
    "        _x = time_series[i:i+seq_length, :]\n",
    "        _y = time_series[i+seq_length, [-1]]\n",
    "        # print(_x, \"-->\",_y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = build_dataset(np.array(train_set), seq_length)\n",
    "testX, testY = build_dataset(np.array(test_set), seq_length)\n",
    "\n",
    "# 텐서로 변환\n",
    "trainX_tensor = torch.FloatTensor(trainX)\n",
    "trainY_tensor = torch.FloatTensor(trainY)\n",
    "testX_tensor = torch.FloatTensor(testX)\n",
    "testY_tensor = torch.FloatTensor(testY)\n",
    "\n",
    "# 텐서 형태로 데이터 정의\n",
    "dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
    "\n",
    "# 데이터로더는 기본적으로 2개의 인자를 입력받으며 배치크기는 통상적으로 2의 배수를 사용\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=True,  \n",
    "                        drop_last=True)\n",
    "\n",
    "# 설정값\n",
    "data_dim = 1\n",
    "hidden_dim = 10 \n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 100\n",
    "class Net(nn.Module):\n",
    "    # # 기본변수, layer를 초기화해주는 생성자\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, output_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=layers,\n",
    "                            # dropout = 0.1,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim, bias = True) \n",
    "        \n",
    "    # 학습 초기화를 위한 함수\n",
    "    def reset_hidden_state(self): \n",
    "        self.hidden = (\n",
    "                torch.zeros(self.layers, self.seq_len, self.hidden_dim),\n",
    "                torch.zeros(self.layers, self.seq_len, self.hidden_dim))\n",
    "    \n",
    "    # 예측을 위한 함수\n",
    "    def forward(self, x):\n",
    "        x, _status = self.lstm(x)\n",
    "        x = self.fc(x[:, -1])\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_df, num_epochs = None, lr = None, verbose = 10, patience = 10):\n",
    "     \n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    nb_epochs = num_epochs\n",
    "    \n",
    "    # epoch마다 loss 저장\n",
    "    train_hist = np.zeros(nb_epochs)\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = len(train_df)\n",
    "        \n",
    "        for batch_idx, samples in enumerate(train_df):\n",
    "\n",
    "            x_train, y_train = samples\n",
    "            x_train=x_train.to(device)\n",
    "            y_train=y_train.to(device)\n",
    "            print(\"x_train\",x_train)\n",
    "            # seq별 hidden state reset\n",
    "            model.reset_hidden_state()\n",
    "            \n",
    "            # H(x) 계산\n",
    "            outputs = model(x_train)\n",
    "                \n",
    "            # cost 계산\n",
    "            loss = criterion(outputs, y_train)                    \n",
    "            \n",
    "            # cost로 H(x) 개선\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_cost += loss/total_batch\n",
    "               \n",
    "        train_hist[epoch] = avg_cost        \n",
    "        \n",
    "        if epoch % verbose == 0:\n",
    "            print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n",
    "            \n",
    "        # patience번째 마다 early stopping 여부 확인\n",
    "        if (epoch % patience == 0) & (epoch != 0):\n",
    "            \n",
    "            # loss가 커졌다면 early stop\n",
    "            if train_hist[epoch-patience] < train_hist[epoch]:\n",
    "                print('\\n Early Stopping')\n",
    "                \n",
    "                break\n",
    "            \n",
    "    return model.eval(), train_hist\n",
    "\n",
    "# 모델 학습\n",
    "net = Net(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)  \n",
    "model, train_hist = train_model(net, dataloader, num_epochs = nb_epochs, lr = learning_rate, verbose = 20, patience = 10)\n",
    "\n",
    "# epoch별 손실값\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 저장    \n",
    "PATH = \"./Timeseries_LSTM_data-02-stock_daily_2.pth\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "model = Net(data_dim, hidden_dim, seq_length, output_dim, 1).to(device)  \n",
    "model.load_state_dict(torch.load(PATH), strict=False)\n",
    "model.eval()\n",
    "\n",
    "# 예측 테스트\n",
    "with torch.no_grad(): \n",
    "    pred = []\n",
    "    for pr in range(len(testX_tensor)):\n",
    "\n",
    "        model.reset_hidden_state()\n",
    "\n",
    "        predicted = model(torch.unsqueeze(testX_tensor[pr], 0).to(device) )\n",
    "        predicted = torch.flatten(predicted).item()\n",
    "        pred.append(predicted)\n",
    "\n",
    "    # INVERSE\n",
    "    pred_inverse = scaler_y.inverse_transform(np.array(pred).reshape(-1, 1))\n",
    "    testY_inverse = scaler_y.inverse_transform(testY_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0da35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(true, pred):\n",
    "    return np.mean(np.abs(true-pred))\n",
    "\n",
    "print('MAE SCORE : ', MAE(pred_inverse, testY_inverse))\n",
    "\n",
    "#MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"MAE\",mean_absolute_error(testY_inverse,pred_inverse))\n",
    "\n",
    "#MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE\",mean_squared_error(testY_inverse,pred_inverse))\n",
    "\n",
    "#RMSE\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MSE = mean_squared_error(testY_inverse,pred_inverse)\n",
    "print(\"MSE\",np.sqrt(MSE))\n",
    "# sklearn 은 mse만 제공하기 때문에 rmse는 직접 만들어 써야한다.\n",
    "\n",
    "#MAPE\n",
    "import numpy as np\n",
    "\n",
    "def MAPE(y_test, y_pred):\n",
    "\treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    \n",
    "print(\"MAPE\",MAPE(testY_inverse, pred_inverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.plot(np.arange(len(pred_inverse)), pred_inverse, label = 'pred')\n",
    "plt.plot(np.arange(len(testY_inverse)), testY_inverse, label = 'true')\n",
    "plt.title(\"Loss plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16794ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad81d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6855c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
